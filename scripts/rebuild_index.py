"""
========================================
é‡å»ºç´¢å¼•è„šæœ¬
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- é‡å»º BM25 ç´¢å¼•
- é‡å»ºå‘é‡ç´¢å¼•
- æ”¯æŒå¢é‡å’Œå…¨é‡é‡å»º

ğŸš€ ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/rebuild_index.py

========================================
"""

import sys
import os
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from loguru import logger

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.config import settings
from core.constants import MilvusCollection

# å¯¼å…¥æœåŠ¡æ¨¡å—
from services.retrieval.bm25.bm25_engine import BM25Retriever
from services.embedding.embedding_model import EmbeddingModel
from services.embedding.embedder import Embedder
from repository.vector_repo import VectorRepository
from repository.document_repo import DocumentRepository

# æ•°æ®åº“
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker


class IndexRebuilder:
    """
    ç´¢å¼•é‡å»ºå™¨

    ğŸ”§ åŠŸèƒ½ï¼š
    1. é‡å»º BM25 ç´¢å¼•
    2. é‡å»ºå‘é‡ç´¢å¼•
    3. æ”¯æŒæŒ‡å®šé›†åˆ
    4. æ”¯æŒå¢é‡æ›´æ–°
    """

    def __init__(self):
        """åˆå§‹åŒ–ç´¢å¼•é‡å»ºå™¨"""
        logger.info("åˆå§‹åŒ–ç´¢å¼•é‡å»ºå™¨...")

        # åˆå§‹åŒ–æ•°æ®åº“
        self.engine = create_engine(settings.postgres_url)
        Session = sessionmaker(bind=self.engine)
        self.session = Session()
        self.doc_repo = DocumentRepository(self.session)

        # åˆå§‹åŒ–å‘é‡åº“
        self.vector_repo = VectorRepository()

        # åˆå§‹åŒ– Embedding
        self.embedding_model = EmbeddingModel(
            model_name=settings.EMBEDDING_MODEL_NAME
        )
        self.embedder = Embedder(
            embedding_model=self.embedding_model,
            batch_size=settings.EMBEDDING_BATCH_SIZE
        )

        # åˆå§‹åŒ– BM25
        self.bm25_retriever = BM25Retriever()

        # ç»Ÿè®¡
        self.stats = {
            'bm25_docs': 0,
            'vector_docs': 0,
            'errors': 0,
            'total_time': 0
        }

        logger.info("ç´¢å¼•é‡å»ºå™¨åˆå§‹åŒ–å®Œæˆ")

    def rebuild_bm25_index(
        self,
        save_path: Optional[str] = None
    ) -> bool:
        """
        é‡å»º BM25 ç´¢å¼•

        å‚æ•°ï¼š
            save_path: ç´¢å¼•ä¿å­˜è·¯å¾„

        è¿”å›ï¼š
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹é‡å»º BM25 ç´¢å¼•")
        logger.info("=" * 60)

        start_time = datetime.now()

        try:
            # 1. ä»æ•°æ®åº“è·å–æ‰€æœ‰æ–‡æ¡£
            logger.info("ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£...")
            documents = self.doc_repo.get_all_documents()

            if not documents:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
                return False

            logger.info(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

            # 2. å‡†å¤‡ BM25 æ–‡æ¡£æ ¼å¼
            bm25_docs = []
            for doc in documents:
                # è·å–æ–‡æ¡£çš„æ‰€æœ‰ chunks
                chunks = self.doc_repo.get_document_chunks(doc.id)

                for chunk in chunks:
                    bm25_docs.append({
                        'id': f"{doc.id}_{chunk.chunk_index}",
                        'text': chunk.text,
                        'metadata': {
                            'doc_id': doc.id,
                            'doc_name': doc.name,
                            'chunk_index': chunk.chunk_index
                        }
                    })

            logger.info(f"å‡†å¤‡äº† {len(bm25_docs)} ä¸ªæ–‡æ¡£å—")

            # 3. æ„å»ºç´¢å¼•
            logger.info("æ„å»º BM25 ç´¢å¼•...")
            self.bm25_retriever.build_index(bm25_docs)

            # 4. ä¿å­˜ç´¢å¼•
            if save_path is None:
                save_path = str(settings.DATA_DIR / "indexes" / "bm25_index.pkl")

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)

            self.bm25_retriever.save(save_path)
            logger.info(f"ç´¢å¼•å·²ä¿å­˜: {save_path}")

            # æ›´æ–°ç»Ÿè®¡
            self.stats['bm25_docs'] = len(bm25_docs)
            process_time = (datetime.now() - start_time).total_seconds()
            self.stats['total_time'] += process_time

            logger.info(f"âœ“ BM25 ç´¢å¼•é‡å»ºå®Œæˆ | æ–‡æ¡£æ•°: {len(bm25_docs)} | è€—æ—¶: {process_time:.2f}s")

            return True

        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"âœ— BM25 ç´¢å¼•é‡å»ºå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def rebuild_vector_index(
        self,
        collection_name: Optional[str] = None,
        drop_existing: bool = False
    ) -> bool:
        """
        é‡å»ºå‘é‡ç´¢å¼•

        å‚æ•°ï¼š
            collection_name: é›†åˆåç§°ï¼ˆNone=é‡å»ºæ‰€æœ‰ï¼‰
            drop_existing: æ˜¯å¦åˆ é™¤ç°æœ‰é›†åˆ

        è¿”å›ï¼š
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹é‡å»ºå‘é‡ç´¢å¼•")
        logger.info("=" * 60)

        start_time = datetime.now()

        try:
            # ç¡®å®šè¦é‡å»ºçš„é›†åˆ
            if collection_name:
                collections = [collection_name]
            else:
                collections = [
                    MilvusCollection.STANDARDS.value,
                    MilvusCollection.PROJECTS.value,
                    MilvusCollection.CONTRACTS.value
                ]

            total_vectors = 0

            for coll_name in collections:
                logger.info(f"\nå¤„ç†é›†åˆ: {coll_name}")

                # åˆ é™¤ç°æœ‰é›†åˆ
                if drop_existing:
                    logger.info(f"  åˆ é™¤ç°æœ‰é›†åˆ...")
                    self.vector_repo.drop_collection(coll_name)

                # åˆ›å»ºé›†åˆ
                logger.info(f"  åˆ›å»ºé›†åˆ...")
                self.vector_repo.create_collection(
                    collection_name=coll_name,
                    description=f"RAG å‘é‡é›†åˆ - {coll_name}"
                )

                # åˆ›å»ºç´¢å¼•
                logger.info(f"  åˆ›å»ºç´¢å¼•...")
                self.vector_repo.create_index(coll_name)

                # è·å–è¯¥é›†åˆçš„æ–‡æ¡£
                documents = self.doc_repo.get_documents_by_collection(coll_name)

                if not documents:
                    logger.info(f"  é›†åˆ {coll_name} æ²¡æœ‰æ–‡æ¡£")
                    continue

                logger.info(f"  æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")

                # é‡æ–°å‘é‡åŒ–å¹¶æ’å…¥
                vectors_data = []
                for doc in documents:
                    chunks = self.doc_repo.get_document_chunks(doc.id)

                    for chunk in chunks:
                        # å‘é‡åŒ–
                        embedding = self.embedder.embed_query(chunk.text)

                        vectors_data.append({
                            'doc_id': f"{doc.id}_{chunk.chunk_index}",
                            'text': chunk.text,
                            'embedding': embedding,
                            'metadata': {
                                'doc_id': doc.id,
                                'doc_name': doc.name,
                                'chunk_index': chunk.chunk_index
                            }
                        })

                # æ‰¹é‡æ’å…¥
                if vectors_data:
                    logger.info(f"  æ’å…¥ {len(vectors_data)} ä¸ªå‘é‡...")
                    self.vector_repo.insert_vectors(
                        collection_name=coll_name,
                        vectors=vectors_data
                    )
                    total_vectors += len(vectors_data)

                logger.info(f"  âœ“ é›†åˆ {coll_name} å®Œæˆ")

            # æ›´æ–°ç»Ÿè®¡
            self.stats['vector_docs'] = total_vectors
            process_time = (datetime.now() - start_time).total_seconds()
            self.stats['total_time'] += process_time

            logger.info(f"\nâœ“ å‘é‡ç´¢å¼•é‡å»ºå®Œæˆ | æ€»å‘é‡æ•°: {total_vectors} | è€—æ—¶: {process_time:.2f}s")

            return True

        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"âœ— å‘é‡ç´¢å¼•é‡å»ºå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def rebuild_all(
        self,
        drop_existing: bool = False
    ) -> bool:
        """
        é‡å»ºæ‰€æœ‰ç´¢å¼•

        å‚æ•°ï¼š
            drop_existing: æ˜¯å¦åˆ é™¤ç°æœ‰æ•°æ®

        è¿”å›ï¼š
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹é‡å»ºæ‰€æœ‰ç´¢å¼•")
        logger.info("=" * 60)

        success = True

        # é‡å»º BM25
        if not self.rebuild_bm25_index():
            success = False

        # é‡å»ºå‘é‡ç´¢å¼•
        if not self.rebuild_vector_index(drop_existing=drop_existing):
            success = False

        return success

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ç´¢å¼•é‡å»ºç»Ÿè®¡")
        print("=" * 60)
        print(f"  BM25 æ–‡æ¡£æ•°: {self.stats['bm25_docs']}")
        print(f"  å‘é‡æ–‡æ¡£æ•°: {self.stats['vector_docs']}")
        print(f"  é”™è¯¯æ•°: {self.stats['errors']}")
        print(f"  æ€»è€—æ—¶: {self.stats['total_time']:.2f}s")
        print("=" * 60)

    def close(self):
        """å…³é—­è¿æ¥"""
        try:
            self.session.close()
            self.vector_repo.disconnect()
            logger.info("è¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­è¿æ¥å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¼ä¸šçº§ RAG ç³»ç»Ÿ - ç´¢å¼•é‡å»ºå·¥å…·"
    )

    parser.add_argument(
        '-t', '--type',
        choices=['all', 'bm25', 'vector'],
        default='all',
        help='é‡å»ºç±»å‹ï¼ˆé»˜è®¤: allï¼‰'
    )

    parser.add_argument(
        '-c', '--collection',
        default=None,
        help='å‘é‡åº“é›†åˆåç§°ï¼ˆä»… vector ç±»å‹æœ‰æ•ˆï¼‰'
    )

    parser.add_argument(
        '--drop',
        action='store_true',
        help='åˆ é™¤ç°æœ‰æ•°æ®åé‡å»º'
    )

    parser.add_argument(
        '-y', '--yes',
        action='store_true',
        help='è·³è¿‡ç¡®è®¤'
    )

    args = parser.parse_args()

    # æ‰“å°é…ç½®
    print("\n" + "=" * 60)
    print("ğŸ”§ ä¼ä¸šçº§ RAG ç³»ç»Ÿ - ç´¢å¼•é‡å»ºå·¥å…·")
    print("=" * 60)
    print(f"  é‡å»ºç±»å‹: {args.type}")
    print(f"  é›†åˆ: {args.collection or 'æ‰€æœ‰'}")
    print(f"  åˆ é™¤ç°æœ‰: {'æ˜¯' if args.drop else 'å¦'}")
    print("=" * 60)

    # ç¡®è®¤
    if not args.yes:
        if args.drop:
            confirm = input("\nâš ï¸ å°†åˆ é™¤ç°æœ‰æ•°æ®ï¼Œç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
        else:
            confirm = input("\nç¡®è®¤å¼€å§‹é‡å»ºï¼Ÿ(y/n): ").strip().lower()

        if confirm != 'y':
            print("å·²å–æ¶ˆ")
            return

    # åˆå§‹åŒ–é‡å»ºå™¨
    rebuilder = IndexRebuilder()

    try:
        if args.type == 'all':
            rebuilder.rebuild_all(drop_existing=args.drop)
        elif args.type == 'bm25':
            rebuilder.rebuild_bm25_index()
        elif args.type == 'vector':
            rebuilder.rebuild_vector_index(
                collection_name=args.collection,
                drop_existing=args.drop
            )

        # æ‰“å°ç»Ÿè®¡
        rebuilder.print_stats()

    finally:
        rebuilder.close()


if __name__ == "__main__":
    main()


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
# 1. é‡å»ºæ‰€æœ‰ç´¢å¼•
python scripts/rebuild_index.py

# 2. åªé‡å»º BM25 ç´¢å¼•
python scripts/rebuild_index.py -t bm25

# 3. åªé‡å»ºå‘é‡ç´¢å¼•
python scripts/rebuild_index.py -t vector

# 4. é‡å»ºæŒ‡å®šé›†åˆçš„å‘é‡ç´¢å¼•
python scripts/rebuild_index.py -t vector -c rag_standards

# 5. åˆ é™¤ç°æœ‰æ•°æ®åé‡å»º
python scripts/rebuild_index.py --drop

# 6. è·³è¿‡ç¡®è®¤
python scripts/rebuild_index.py -y

# 7. åœ¨ä»£ç ä¸­ä½¿ç”¨
from scripts.rebuild_index import IndexRebuilder

rebuilder = IndexRebuilder()
rebuilder.rebuild_all()
rebuilder.print_stats()
rebuilder.close()
"""
