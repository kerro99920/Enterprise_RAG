"""
========================================
æ–‡æ¡£å…¥åº“è„šæœ¬
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- æ‰¹é‡å¤„ç†æ–‡æ¡£å¹¶å…¥åº“
- æ”¯æŒå¢é‡æ›´æ–°
- å®Œæ•´çš„å¤„ç†æµç¨‹

ğŸš€ ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/ingest_docs.py

========================================
"""

import sys
import os
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import argparse
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from loguru import logger

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.config import settings
from core.constants import DocumentType, DocumentStatus, MilvusCollection

# å¯¼å…¥æœåŠ¡æ¨¡å—
from services.document.loader import DocumentLoader
from services.document.cleaner import DocumentCleaner
from services.document.splitter import DocumentSplitter
from services.document.metadata import MetadataExtractor
from services.embedding.embedding_model import EmbeddingModel
from services.embedding.embedder import Embedder
from repository.vector_repo import VectorRepository
from repository.document_repo import DocumentRepository

# æ•°æ®åº“
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker


class DocumentIngester:
    """
    æ–‡æ¡£å…¥åº“å™¨

    ğŸ”§ å¤„ç†æµç¨‹ï¼š
    1. åŠ è½½æ–‡æ¡£ï¼ˆPDF/Word/å›¾ç‰‡ï¼‰
    2. æ–‡æœ¬æ¸…æ´—
    3. æ–‡æ¡£åˆ†å—
    4. å…ƒæ•°æ®æå–
    5. å‘é‡åŒ–
    6. å­˜å…¥å‘é‡åº“
    7. å­˜å…¥å…³ç³»æ•°æ®åº“
    """

    def __init__(
        self,
        enable_ocr: bool = True,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        batch_size: int = 32
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£å…¥åº“å™¨

        å‚æ•°ï¼š
            enable_ocr: æ˜¯å¦å¯ç”¨ OCR
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
            batch_size: å‘é‡åŒ–æ‰¹å¤§å°
        """
        logger.info("åˆå§‹åŒ–æ–‡æ¡£å…¥åº“å™¨...")

        # åˆå§‹åŒ–ç»„ä»¶
        self.loader = DocumentLoader(enable_ocr=enable_ocr)
        self.cleaner = DocumentCleaner()
        self.splitter = DocumentSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.metadata_extractor = MetadataExtractor()

        # åˆå§‹åŒ– Embedding
        self.embedding_model = EmbeddingModel(
            model_name=settings.EMBEDDING_MODEL_NAME
        )
        self.embedder = Embedder(
            embedding_model=self.embedding_model,
            batch_size=batch_size
        )

        # åˆå§‹åŒ– Repository
        self.vector_repo = VectorRepository()

        # åˆå§‹åŒ–æ•°æ®åº“
        self.engine = create_engine(settings.postgres_url)
        Session = sessionmaker(bind=self.engine)
        self.session = Session()
        self.doc_repo = DocumentRepository(self.session)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_files': 0,
            'processed': 0,
            'failed': 0,
            'total_chunks': 0,
            'total_time': 0
        }

        logger.info("æ–‡æ¡£å…¥åº“å™¨åˆå§‹åŒ–å®Œæˆ")

    def ingest_file(
        self,
        file_path: str,
        collection_name: str = None,
        doc_type: DocumentType = None
    ) -> Dict:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶

        å‚æ•°ï¼š
            file_path: æ–‡ä»¶è·¯å¾„
            collection_name: å‘é‡åº“é›†åˆåç§°
            doc_type: æ–‡æ¡£ç±»å‹

        è¿”å›ï¼š
            å¤„ç†ç»“æœ
        """
        start_time = datetime.now()
        file_name = os.path.basename(file_path)

        logger.info(f"å¼€å§‹å¤„ç†: {file_name}")

        try:
            # 1. åŠ è½½æ–‡æ¡£
            logger.debug(f"  [1/6] åŠ è½½æ–‡æ¡£...")
            doc_data = self.loader.load(file_path)

            # 2. æ¸…æ´—æ–‡æœ¬
            logger.debug(f"  [2/6] æ¸…æ´—æ–‡æœ¬...")
            cleaned_text = self.cleaner.clean(doc_data['text'])

            # 3. æå–å…ƒæ•°æ®
            logger.debug(f"  [3/6] æå–å…ƒæ•°æ®...")
            metadata = self.metadata_extractor.extract(
                cleaned_text,
                file_path=file_path,
                doc_metadata=doc_data.get('metadata', {})
            )

            # 4. æ–‡æ¡£åˆ†å—
            logger.debug(f"  [4/6] æ–‡æ¡£åˆ†å—...")
            chunks = self.splitter.split(
                cleaned_text,
                method='recursive',
                metadata=metadata
            )

            if not chunks:
                raise ValueError("æ–‡æ¡£åˆ†å—å¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•å—")

            logger.debug(f"    ç”Ÿæˆ {len(chunks)} ä¸ªå—")

            # 5. å‘é‡åŒ–
            logger.debug(f"  [5/6] å‘é‡åŒ–...")
            embedded_chunks = self.embedder.embed_chunks(chunks)

            # 6. å­˜å…¥å‘é‡åº“
            logger.debug(f"  [6/6] å­˜å…¥å‘é‡åº“...")

            # ç¡®å®šé›†åˆåç§°
            if collection_name is None:
                collection_name = self._determine_collection(doc_type, metadata)

            # å‡†å¤‡å‘é‡æ•°æ®
            vectors_data = []
            for i, chunk in enumerate(embedded_chunks):
                vectors_data.append({
                    'doc_id': f"{file_name}_{i}",
                    'text': chunk['text'],
                    'embedding': chunk['embedding'],
                    'metadata': {
                        'file_name': file_name,
                        'file_path': file_path,
                        'chunk_index': i,
                        'total_chunks': len(embedded_chunks),
                        **chunk.get('metadata', {})
                    }
                })

            # æ’å…¥å‘é‡åº“
            self.vector_repo.insert_vectors(
                collection_name=collection_name,
                vectors=vectors_data
            )

            # 7. å­˜å…¥å…³ç³»æ•°æ®åº“
            doc_record = self.doc_repo.create_document(
                name=file_name,
                doc_type=doc_type or DocumentType.OTHER,
                source_path=file_path,
                status=DocumentStatus.COMPLETED,
                total_chunks=len(chunks),
                vector_collection=collection_name,
                embedding_model=settings.EMBEDDING_MODEL_NAME,
                **{k: v for k, v in metadata.items() if k in ['title', 'author', 'version']}
            )

            # è®¡ç®—è€—æ—¶
            process_time = (datetime.now() - start_time).total_seconds()

            # æ›´æ–°ç»Ÿè®¡
            self.stats['processed'] += 1
            self.stats['total_chunks'] += len(chunks)
            self.stats['total_time'] += process_time

            result = {
                'success': True,
                'file_name': file_name,
                'doc_id': doc_record.id,
                'chunks': len(chunks),
                'collection': collection_name,
                'process_time': process_time
            }

            logger.info(
                f"âœ“ å¤„ç†å®Œæˆ: {file_name} | "
                f"å—æ•°: {len(chunks)} | "
                f"è€—æ—¶: {process_time:.2f}s"
            )

            return result

        except Exception as e:
            self.stats['failed'] += 1

            logger.error(f"âœ— å¤„ç†å¤±è´¥: {file_name} | é”™è¯¯: {str(e)}")

            return {
                'success': False,
                'file_name': file_name,
                'error': str(e)
            }

    def ingest_directory(
        self,
        directory: str,
        collection_name: str = None,
        recursive: bool = True,
        file_types: List[str] = None
    ) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ–‡æ¡£

        å‚æ•°ï¼š
            directory: ç›®å½•è·¯å¾„
            collection_name: å‘é‡åº“é›†åˆåç§°
            recursive: æ˜¯å¦é€’å½’å­ç›®å½•
            file_types: é™å®šæ–‡ä»¶ç±»å‹ï¼ˆå¦‚ ['.pdf', '.docx']ï¼‰

        è¿”å›ï¼š
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        if not os.path.isdir(directory):
            raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {directory}")

        # æ”¶é›†æ–‡ä»¶
        files = []
        supported_types = file_types or self.loader.get_supported_formats()

        if recursive:
            for root, dirs, filenames in os.walk(directory):
                for filename in filenames:
                    if any(filename.lower().endswith(ext) for ext in supported_types):
                        files.append(os.path.join(root, filename))
        else:
            for filename in os.listdir(directory):
                file_path = os.path.join(directory, filename)
                if os.path.isfile(file_path):
                    if any(filename.lower().endswith(ext) for ext in supported_types):
                        files.append(file_path)

        self.stats['total_files'] = len(files)

        logger.info(f"æ‰¾åˆ° {len(files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")

        # æ‰¹é‡å¤„ç†
        results = []
        for i, file_path in enumerate(files, 1):
            logger.info(f"[{i}/{len(files)}] å¤„ç†ä¸­...")
            result = self.ingest_file(file_path, collection_name)
            results.append(result)

        return results

    def _determine_collection(
        self,
        doc_type: Optional[DocumentType],
        metadata: Dict
    ) -> str:
        """
        æ ¹æ®æ–‡æ¡£ç±»å‹ç¡®å®šå‘é‡åº“é›†åˆ

        è§„åˆ™ï¼š
        - æ ‡å‡†/è§„èŒƒç±» -> rag_standards
        - é¡¹ç›®èµ„æ–™ç±» -> rag_projects
        - åˆåŒç±» -> rag_contracts
        - å…¶ä»– -> rag_projectsï¼ˆé»˜è®¤ï¼‰
        """
        if doc_type == DocumentType.STANDARD:
            return MilvusCollection.STANDARDS.value
        elif doc_type == DocumentType.CONTRACT:
            return MilvusCollection.CONTRACTS.value
        elif doc_type == DocumentType.PROJECT:
            return MilvusCollection.PROJECTS.value

        # æ ¹æ®å…ƒæ•°æ®åˆ¤æ–­
        title = metadata.get('title', '').lower()
        doc_number = metadata.get('document_number', '')

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡å‡†/è§„èŒƒ
        if doc_number and any(prefix in doc_number.upper() for prefix in ['GB', 'JGJ', 'CJJ']):
            return MilvusCollection.STANDARDS.value

        if any(keyword in title for keyword in ['è§„èŒƒ', 'æ ‡å‡†', 'è§„ç¨‹', 'standard']):
            return MilvusCollection.STANDARDS.value

        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆåŒ
        if any(keyword in title for keyword in ['åˆåŒ', 'åè®®', 'contract']):
            return MilvusCollection.CONTRACTS.value

        # é»˜è®¤é¡¹ç›®èµ„æ–™åº“
        return MilvusCollection.PROJECTS.value

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š å…¥åº“ç»Ÿè®¡")
        print("=" * 60)
        print(f"  æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"  æˆåŠŸå¤„ç†: {self.stats['processed']}")
        print(f"  å¤„ç†å¤±è´¥: {self.stats['failed']}")
        print(f"  æ€»å—æ•°: {self.stats['total_chunks']}")
        print(f"  æ€»è€—æ—¶: {self.stats['total_time']:.2f}s")

        if self.stats['processed'] > 0:
            avg_time = self.stats['total_time'] / self.stats['processed']
            avg_chunks = self.stats['total_chunks'] / self.stats['processed']
            print(f"  å¹³å‡è€—æ—¶: {avg_time:.2f}s/æ–‡ä»¶")
            print(f"  å¹³å‡å—æ•°: {avg_chunks:.1f}å—/æ–‡ä»¶")

        print("=" * 60)

    def close(self):
        """å…³é—­è¿æ¥"""
        try:
            self.session.close()
            self.vector_repo.disconnect()
            logger.info("è¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­è¿æ¥å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¼ä¸šçº§ RAG ç³»ç»Ÿ - æ–‡æ¡£å…¥åº“å·¥å…·"
    )

    parser.add_argument(
        'path',
        nargs='?',
        default=str(settings.RAW_DOCS_DIR),
        help='æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: data/raw_docsï¼‰'
    )

    parser.add_argument(
        '-c', '--collection',
        default=None,
        help='å‘é‡åº“é›†åˆåç§°'
    )

    parser.add_argument(
        '-t', '--type',
        choices=['standard', 'project', 'contract', 'other'],
        default=None,
        help='æ–‡æ¡£ç±»å‹'
    )

    parser.add_argument(
        '--no-recursive',
        action='store_true',
        help='ä¸é€’å½’å¤„ç†å­ç›®å½•'
    )

    parser.add_argument(
        '--no-ocr',
        action='store_true',
        help='ç¦ç”¨ OCR'
    )

    parser.add_argument(
        '--chunk-size',
        type=int,
        default=500,
        help='åˆ†å—å¤§å°ï¼ˆé»˜è®¤: 500ï¼‰'
    )

    parser.add_argument(
        '--batch-size',
        type=int,
        default=32,
        help='å‘é‡åŒ–æ‰¹å¤§å°ï¼ˆé»˜è®¤: 32ï¼‰'
    )

    args = parser.parse_args()

    # æ‰“å°é…ç½®
    print("\n" + "=" * 60)
    print("ğŸš€ ä¼ä¸šçº§ RAG ç³»ç»Ÿ - æ–‡æ¡£å…¥åº“å·¥å…·")
    print("=" * 60)
    print(f"  è·¯å¾„: {args.path}")
    print(f"  é›†åˆ: {args.collection or 'è‡ªåŠ¨åˆ¤æ–­'}")
    print(f"  ç±»å‹: {args.type or 'è‡ªåŠ¨åˆ¤æ–­'}")
    print(f"  OCR: {'ç¦ç”¨' if args.no_ocr else 'å¯ç”¨'}")
    print(f"  é€’å½’: {'å¦' if args.no_recursive else 'æ˜¯'}")
    print(f"  åˆ†å—å¤§å°: {args.chunk_size}")
    print("=" * 60)

    # ç¡®è®¤
    confirm = input("\nç¡®è®¤å¼€å§‹å¤„ç†ï¼Ÿ(y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    # åˆå§‹åŒ–å…¥åº“å™¨
    ingester = DocumentIngester(
        enable_ocr=not args.no_ocr,
        chunk_size=args.chunk_size,
        batch_size=args.batch_size
    )

    try:
        # å¤„ç†æ–‡æ¡£ç±»å‹
        doc_type = None
        if args.type:
            type_map = {
                'standard': DocumentType.STANDARD,
                'project': DocumentType.PROJECT,
                'contract': DocumentType.CONTRACT,
                'other': DocumentType.OTHER
            }
            doc_type = type_map.get(args.type)

        # åˆ¤æ–­æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
        if os.path.isfile(args.path):
            # å•ä¸ªæ–‡ä»¶
            result = ingester.ingest_file(
                args.path,
                collection_name=args.collection,
                doc_type=doc_type
            )
            if result['success']:
                print(f"\nâœ“ æ–‡ä»¶å¤„ç†æˆåŠŸ: {result['file_name']}")
            else:
                print(f"\nâœ— æ–‡ä»¶å¤„ç†å¤±è´¥: {result['error']}")

        elif os.path.isdir(args.path):
            # ç›®å½•
            results = ingester.ingest_directory(
                args.path,
                collection_name=args.collection,
                recursive=not args.no_recursive
            )

            # æ‰“å°ç»“æœæ‘˜è¦
            success_count = sum(1 for r in results if r['success'])
            fail_count = sum(1 for r in results if not r['success'])

            print(f"\nå¤„ç†å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")

            # æ‰“å°å¤±è´¥æ–‡ä»¶
            if fail_count > 0:
                print("\nå¤±è´¥æ–‡ä»¶:")
                for r in results:
                    if not r['success']:
                        print(f"  - {r['file_name']}: {r['error']}")

        else:
            print(f"è·¯å¾„ä¸å­˜åœ¨: {args.path}")
            return

        # æ‰“å°ç»Ÿè®¡
        ingester.print_stats()

    finally:
        ingester.close()


if __name__ == "__main__":
    main()


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
# 1. å¤„ç†é»˜è®¤ç›®å½•ï¼ˆdata/raw_docsï¼‰
python scripts/ingest_docs.py

# 2. å¤„ç†æŒ‡å®šç›®å½•
python scripts/ingest_docs.py /path/to/docs

# 3. å¤„ç†å•ä¸ªæ–‡ä»¶
python scripts/ingest_docs.py /path/to/doc.pdf

# 4. æŒ‡å®šæ–‡æ¡£ç±»å‹
python scripts/ingest_docs.py -t standard /path/to/standards

# 5. æŒ‡å®šå‘é‡åº“é›†åˆ
python scripts/ingest_docs.py -c rag_standards /path/to/standards

# 6. ç¦ç”¨ OCRï¼ˆåŠ å¿«å¤„ç†é€Ÿåº¦ï¼‰
python scripts/ingest_docs.py --no-ocr /path/to/docs

# 7. ä¸é€’å½’å­ç›®å½•
python scripts/ingest_docs.py --no-recursive /path/to/docs

# 8. è‡ªå®šä¹‰åˆ†å—å¤§å°
python scripts/ingest_docs.py --chunk-size 300 /path/to/docs

# 9. åœ¨ä»£ç ä¸­ä½¿ç”¨
from scripts.ingest_docs import DocumentIngester

ingester = DocumentIngester()
result = ingester.ingest_file("document.pdf")
print(result)
"""
