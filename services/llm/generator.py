"""
========================================
RAGç­”æ¡ˆç”Ÿæˆå™¨
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- æ•´åˆæ£€ç´¢å’Œç”Ÿæˆçš„å®Œæ•´RAGæµç¨‹
- æ”¯æŒå¤šç§ç”Ÿæˆç­–ç•¥
- æä¾›ä¸°å¯Œçš„ç­”æ¡ˆå…ƒæ•°æ®

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ£€ç´¢å¢å¼ºç”Ÿæˆ
2. ä¸Šä¸‹æ–‡ç®¡ç†
3. ç­”æ¡ˆè¯„ä¼°
4. æµå¼è¾“å‡º

========================================
"""

from typing import List, Dict, Optional, Generator, Union
from datetime import datetime

from loguru import logger

from services.llm.llm_client import LLMClient
from services.llm.prompt.qa_prompt import QAPromptFactory
from services.retrieval.hybrid_retriever import HybridRetriever


class AnswerGenerator:
    """
    RAGç­”æ¡ˆç”Ÿæˆå™¨

    ğŸ”§ æ ¸å¿ƒæµç¨‹ï¼š
    1. æ¥æ”¶ç”¨æˆ·é—®é¢˜
    2. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    3. æ„å»ºPrompt
    4. LLMç”Ÿæˆç­”æ¡ˆ
    5. è¿”å›ç»“æœå’Œå…ƒæ•°æ®

    ğŸ’¡ ç‰¹æ€§ï¼š
    - è‡ªåŠ¨ä¸Šä¸‹æ–‡ç®¡ç†
    - å¤šè½®å¯¹è¯æ”¯æŒ
    - ç­”æ¡ˆè´¨é‡è¯„ä¼°
    - å¼•ç”¨è¿½è¸ª
    """

    def __init__(
            self,
            llm_client: LLMClient,
            retriever: HybridRetriever,
            language: str = 'zh',
            default_top_k: int = 5,
            max_context_length: int = 3000
    ):
        """
        åˆå§‹åŒ–ç­”æ¡ˆç”Ÿæˆå™¨

        å‚æ•°ï¼š
            llm_client: LLMå®¢æˆ·ç«¯
            retriever: æ··åˆæ£€ç´¢å™¨
            language: è¯­è¨€
            default_top_k: é»˜è®¤æ£€ç´¢æ•°é‡
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        """
        self.llm_client = llm_client
        self.retriever = retriever
        self.language = language
        self.default_top_k = default_top_k
        self.max_context_length = max_context_length

        logger.info(
            f"ç­”æ¡ˆç”Ÿæˆå™¨åˆå§‹åŒ– | "
            f"è¯­è¨€: {language} | "
            f"top_k: {default_top_k}"
        )

    def generate(
            self,
            query: str,
            top_k: Optional[int] = None,
            use_rerank: bool = True,
            stream: bool = False,
            prompt_type: str = 'rag',
            include_sources: bool = True,
            **retrieval_kwargs
    ) -> Union[Dict, Generator[str, None, None]]:
        """
        ç”Ÿæˆç­”æ¡ˆ

        å‚æ•°ï¼š
            query: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢æ•°é‡
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            stream: æ˜¯å¦æµå¼è¾“å‡º
            prompt_type: Promptç±»å‹ ('rag', 'citation', 'explanation')
            include_sources: æ˜¯å¦åŒ…å«æ¥æºä¿¡æ¯
            **retrieval_kwargs: ä¼ é€’ç»™æ£€ç´¢å™¨çš„å…¶ä»–å‚æ•°

        è¿”å›ï¼š
            - stream=False: å®Œæ•´ç­”æ¡ˆå­—å…¸
                {
                    'answer': str,           # ç­”æ¡ˆæ–‡æœ¬
                    'sources': List[Dict],   # æ¥æºæ–‡æ¡£
                    'query': str,            # åŸå§‹é—®é¢˜
                    'metadata': Dict         # å…ƒæ•°æ®
                }
            - stream=True: æ–‡æœ¬ç”Ÿæˆå™¨
        """
        logger.info(f"ç”Ÿæˆç­”æ¡ˆ | é—®é¢˜: {query[:50]}... | æµå¼: {stream}")

        start_time = datetime.now()

        # Step 1: æ£€ç´¢ç›¸å…³æ–‡æ¡£
        if top_k is None:
            top_k = self.default_top_k

        logger.debug(f"æ£€ç´¢æ–‡æ¡£ | top_k: {top_k}")

        retrieved_docs = self.retriever.search(
            query=query,
            top_k=top_k,
            use_rerank=use_rerank,
            **retrieval_kwargs
        )

        if not retrieved_docs:
            logger.warning("æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
            return self._generate_no_context_answer(query, stream)

        logger.info(f"æ£€ç´¢å®Œæˆ | æ–‡æ¡£æ•°: {len(retrieved_docs)}")

        # Step 2: æ„å»ºPrompt
        prompt = QAPromptFactory.build_rag_prompt(
            query=query,
            contexts=[
                {
                    'text': doc.get('text', ''),
                    'metadata': {
                        'source': doc.get('doc_id', 'Unknown'),
                        'score': doc.get('rerank_score', doc.get('score', 0))
                    }
                }
                for doc in retrieved_docs
            ],
            language=self.language,
            max_context_length=self.max_context_length,
            include_metadata=include_sources
        )

        logger.debug(f"Prompté•¿åº¦: {len(prompt)}")

        # Step 3: LLMç”Ÿæˆç­”æ¡ˆ
        if stream:
            # æµå¼è¾“å‡ºï¼ˆåªè¿”å›æ–‡æœ¬ç”Ÿæˆå™¨ï¼‰
            return self.llm_client.generate(
                prompt=prompt,
                stream=True
            )
        else:
            # éæµå¼è¾“å‡ºï¼ˆè¿”å›å®Œæ•´ç»“æœï¼‰
            answer = self.llm_client.generate(
                prompt=prompt,
                stream=False
            )

            # Step 4: æ„å»ºå“åº”
            end_time = datetime.now()
            response_time = (end_time - start_time).total_seconds()

            result = {
                'answer': answer,
                'query': query,
                'sources': retrieved_docs if include_sources else [],
                'metadata': {
                    'retrieved_docs': len(retrieved_docs),
                    'response_time': response_time,
                    'timestamp': end_time.isoformat(),
                    'model': self.llm_client.model,
                    'language': self.language,
                    'prompt_type': prompt_type,
                    'used_rerank': use_rerank
                }
            }

            logger.info(
                f"ç­”æ¡ˆç”Ÿæˆå®Œæˆ | "
                f"è€—æ—¶: {response_time:.2f}s | "
                f"ç­”æ¡ˆé•¿åº¦: {len(answer)}"
            )

            return result

    def _generate_no_context_answer(
            self,
            query: str,
            stream: bool
    ) -> Union[Dict, Generator[str, None, None]]:
        """
        æ— ä¸Šä¸‹æ–‡æ—¶çš„ç­”æ¡ˆç”Ÿæˆ

        å‘ŠçŸ¥ç”¨æˆ·æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯
        """
        if self.language == 'zh':
            fallback_message = (
                f"æŠ±æ­‰ï¼Œæˆ‘åœ¨ç°æœ‰çš„çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸'{query}'ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚\n\n"
                "å»ºè®®ï¼š\n"
                "1. å°è¯•ç”¨ä¸åŒçš„æ–¹å¼è¡¨è¿°é—®é¢˜\n"
                "2. æ£€æŸ¥é—®é¢˜ä¸­çš„ä¸“ä¸šæœ¯è¯­æ˜¯å¦å‡†ç¡®\n"
                "3. å¦‚æœæ˜¯ç‰¹å®šè§„èŒƒæˆ–æ ‡å‡†ï¼Œè¯·ç¡®è®¤å…¶å·²è¢«æ”¶å½•åˆ°çŸ¥è¯†åº“ä¸­"
            )
        else:
            fallback_message = (
                f"Sorry, I couldn't find relevant information for '{query}' in the knowledge base.\n\n"
                "Suggestions:\n"
                "1. Try rephrasing your question\n"
                "2. Check if technical terms are accurate\n"
                "3. Ensure the specific regulation or standard is included in the knowledge base"
            )

        if stream:
            def fallback_generator():
                yield fallback_message

            return fallback_generator()
        else:
            return {
                'answer': fallback_message,
                'query': query,
                'sources': [],
                'metadata': {
                    'retrieved_docs': 0,
                    'no_context': True,
                    'timestamp': datetime.now().isoformat()
                }
            }

    def chat(
            self,
            query: str,
            conversation_history: Optional[List[Dict]] = None,
            top_k: Optional[int] = None,
            **kwargs
    ) -> Dict:
        """
        å¤šè½®å¯¹è¯ç”Ÿæˆ

        å‚æ•°ï¼š
            query: å½“å‰é—®é¢˜
            conversation_history: å¯¹è¯å†å²
                [
                    {"role": "user", "content": "é—®é¢˜1"},
                    {"role": "assistant", "content": "ç­”æ¡ˆ1"},
                    ...
                ]
            top_k: æ£€ç´¢æ•°é‡
            **kwargs: å…¶ä»–å‚æ•°

        è¿”å›ï¼š
            ç­”æ¡ˆå­—å…¸
        """
        logger.info(f"å¤šè½®å¯¹è¯ | å†å²è½®æ•°: {len(conversation_history or []) // 2}")

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        if top_k is None:
            top_k = self.default_top_k

        retrieved_docs = self.retriever.search(
            query=query,
            top_k=top_k,
            **kwargs
        )

        # æ„å»ºä¸Šä¸‹æ–‡
        context = '\n\n'.join([
            f"ã€æ–‡æ¡£{i + 1}ã€‘\n{doc.get('text', '')}"
            for i, doc in enumerate(retrieved_docs)
        ])

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []

        # ç³»ç»ŸPrompt
        if self.language == 'zh':
            system_content = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥ç¨‹æŠ€æœ¯åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼š

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºå‚è€ƒèµ„æ–™å‡†ç¡®å›ç­”
2. å¦‚æœèµ„æ–™ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
3. ä¿æŒå›ç­”ç®€æ´ä¸“ä¸š"""
        else:
            system_content = f"""You are a professional engineering assistant. Answer based on:

ã€Referencesã€‘
{context}

Requirements:
1. Answer accurately based on references
2. Clearly state if information insufficient
3. Keep answers concise and professional"""

        messages.append({
            "role": "system",
            "content": system_content
        })

        # æ·»åŠ å¯¹è¯å†å²
        if conversation_history:
            messages.extend(conversation_history)

        # æ·»åŠ å½“å‰é—®é¢˜
        messages.append({
            "role": "user",
            "content": query
        })

        # LLMç”Ÿæˆ
        answer = self.llm_client.chat(messages=messages)

        return {
            'answer': answer,
            'query': query,
            'sources': retrieved_docs,
            'conversation_history': messages,
            'metadata': {
                'retrieved_docs': len(retrieved_docs),
                'history_turns': len(conversation_history or []) // 2,
                'timestamp': datetime.now().isoformat()
            }
        }

    def evaluate_answer(self, result: Dict) -> Dict:
        """
        è¯„ä¼°ç­”æ¡ˆè´¨é‡

        å‚æ•°ï¼š
            result: generate()è¿”å›çš„ç»“æœ

        è¿”å›ï¼š
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        answer = result.get('answer', '')
        sources = result.get('sources', [])

        # ç®€å•çš„è´¨é‡æŒ‡æ ‡
        metrics = {
            'answer_length': len(answer),
            'has_sources': len(sources) > 0,
            'num_sources': len(sources),
            'avg_source_score': (
                sum(s.get('score', 0) for s in sources) / len(sources)
                if sources else 0
            ),
            'is_fallback': result.get('metadata', {}).get('no_context', False)
        }

        # è´¨é‡è¯„åˆ†ï¼ˆ0-1ï¼‰
        quality_score = 0.0
        if metrics['has_sources']:
            quality_score += 0.3
        if metrics['answer_length'] > 50:
            quality_score += 0.3
        if metrics['avg_source_score'] > 0.7:
            quality_score += 0.4

        metrics['quality_score'] = quality_score

        return metrics


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from services.llm.generator import AnswerGenerator
from services.llm.llm_client import LLMClient
from services.retrieval.hybrid_retriever import HybridRetriever

# 1. åˆå§‹åŒ–ç»„ä»¶
llm_client = LLMClient(
    api_base="http://localhost:8000/v1",
    model="qwen-plus"
)

hybrid_retriever = HybridRetriever(
    bm25_retriever=bm25,
    vector_retriever=vector,
    reranker=reranker
)

generator = AnswerGenerator(
    llm_client=llm_client,
    retriever=hybrid_retriever,
    language='zh'
)

# 2. ç”Ÿæˆç­”æ¡ˆ
result = generator.generate(
    query="å»ºç­‘ç»“æ„è·è½½å¦‚ä½•è®¡ç®—ï¼Ÿ",
    top_k=5,
    use_rerank=True
)

print(f"é—®é¢˜: {result['query']}")
print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"æ¥æºæ•°: {len(result['sources'])}")
print(f"è€—æ—¶: {result['metadata']['response_time']:.2f}s")


# 3. æµå¼è¾“å‡º
print("æµå¼ç­”æ¡ˆ: ", end="", flush=True)
for chunk in generator.generate(
    query="ä»€ä¹ˆæ˜¯æ¥¼é¢æ´»è·è½½ï¼Ÿ",
    stream=True
):
    print(chunk, end="", flush=True)
print()


# 4. å¤šè½®å¯¹è¯
conversation = []

# ç¬¬ä¸€è½®
result1 = generator.chat(
    query="ä»€ä¹ˆæ˜¯æ’è·è½½ï¼Ÿ",
    conversation_history=conversation
)

conversation.extend([
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯æ’è·è½½ï¼Ÿ"},
    {"role": "assistant", "content": result1['answer']}
])

# ç¬¬äºŒè½®
result2 = generator.chat(
    query="å®ƒå’Œæ´»è·è½½æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
    conversation_history=conversation
)

print(f"ç­”æ¡ˆ: {result2['answer']}")


# 5. è¯„ä¼°ç­”æ¡ˆè´¨é‡
metrics = generator.evaluate_answer(result)
print(f"ç­”æ¡ˆè´¨é‡: {metrics}")


# 6. å¸¦å¼•ç”¨çš„ç”Ÿæˆ
result = generator.generate(
    query="åŠå…¬å®¤æ¥¼é¢è·è½½æ ‡å‡†å€¼æ˜¯å¤šå°‘ï¼Ÿ",
    prompt_type='citation',
    include_sources=True
)
"""