"""
========================================
é—®ç­”ç³»ç»ŸPromptæ¨¡æ¿
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- RAGé—®ç­”ä¸“ç”¨Prompt
- ä¼˜åŒ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç­–ç•¥
- æ”¯æŒå¤šç§é—®ç­”åœºæ™¯

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ ‡å‡†RAGé—®ç­”
2. å¼•ç”¨å¼å›ç­”
3. å¯¹æ¯”åˆ†æ
4. è§„èŒƒè§£è¯»

========================================
"""

from typing import List, Dict, Optional
from services.llm.prompt.base_prompt import BasePrompt, PromptBuilder


class RAGPrompt(BasePrompt):
    """
    RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰é—®ç­”Prompt

    ä¸“ä¸ºåŸºäºæ£€ç´¢å†…å®¹çš„é—®ç­”è®¾è®¡
    """

    @property
    def template(self) -> str:
        if self.language == 'zh':
            return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥ç¨‹æŠ€æœ¯åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
${context}

ã€å›ç­”è¦æ±‚ã€‘
1. å¿…é¡»åŸºäºä¸Šè¿°å‚è€ƒèµ„æ–™å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¼•ç”¨å…·ä½“å†…å®¹æ—¶ï¼Œè¯·æ ‡æ³¨æ¥æºï¼ˆå¦‚æ–‡æ¡£åã€ç« èŠ‚å·ï¼‰
4. å›ç­”è¦å‡†ç¡®ã€ä¸“ä¸šã€æ˜“æ‡‚
5. å¦‚æœ‰å¿…è¦ï¼Œå¯ä»¥é€‚å½“è¡¥å……ç›¸å…³èƒŒæ™¯çŸ¥è¯†

ã€ç”¨æˆ·é—®é¢˜ã€‘
${query}

ã€ä½ çš„å›ç­”ã€‘"""
        else:  # en
            return """You are a professional engineering assistant. Please answer user questions based on the following reference materials.

ã€Reference Materialsã€‘
${context}

ã€Answer Requirementsã€‘
1. Must base answer on the above reference materials, do not fabricate
2. If no relevant information in references, clearly state so
3. When citing content, indicate source (document name, section number)
4. Answer should be accurate, professional, and understandable
5. Can supplement relevant background knowledge if necessary

ã€User Questionã€‘
${query}

ã€Your Answerã€‘"""

    @property
    def required_variables(self) -> List[str]:
        return ['context', 'query']


class CitationPrompt(BasePrompt):
    """
    å¼•ç”¨å¼å›ç­”Prompt

    è¦æ±‚æ¨¡å‹åœ¨å›ç­”ä¸­æ˜ç¡®æ ‡æ³¨å¼•ç”¨æ¥æº
    """

    @property
    def template(self) -> str:
        if self.language == 'zh':
            return """è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ï¼Œå¹¶åœ¨å¼•ç”¨æ—¶æ ‡æ³¨æ¥æºã€‚

ã€å‚è€ƒæ–‡æ¡£ã€‘
${context}

ã€å›ç­”æ ¼å¼ã€‘
è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼æ ‡æ³¨å¼•ç”¨ï¼š
- ç›´æ¥å¼•ç”¨ï¼šæ ¹æ®ã€Šæ–‡æ¡£åã€‹ç¬¬Xç« ç¬¬YèŠ‚ï¼š"åŸæ–‡å†…å®¹"
- é—´æ¥å¼•ç”¨ï¼šå‚è€ƒã€Šæ–‡æ¡£åã€‹ç›¸å…³å†…å®¹ï¼Œ...

ã€ç”¨æˆ·é—®é¢˜ã€‘
${query}

ã€å›ç­”ã€‘
è¯·é€ç‚¹å›ç­”ï¼Œå¹¶æ˜ç¡®æ ‡æ³¨æ¯ä¸ªè¦ç‚¹çš„å‡ºå¤„ï¼š"""
        else:
            return """Please answer based on the following reference documents and cite sources.

ã€Reference Documentsã€‘
${context}

ã€Answer Formatã€‘
Use the following format for citations:
- Direct quote: According to "Document Name" Chapter X Section Y: "original text"
- Indirect reference: Referring to "Document Name", ...

ã€User Questionã€‘
${query}

ã€Answerã€‘
Please answer point by point and clearly cite sources for each point:"""

    @property
    def required_variables(self) -> List[str]:
        return ['context', 'query']


class ComparisonPrompt(BasePrompt):
    """
    å¯¹æ¯”åˆ†æPrompt

    ç”¨äºå¯¹æ¯”ä¸åŒè§„èŒƒã€æ ‡å‡†æˆ–æ–¹æ³•
    """

    @property
    def template(self) -> str:
        if self.language == 'zh':
            return """è¯·åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™ï¼Œå¯¹æ¯”åˆ†æ${comparison_target}ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
${context}

ã€å¯¹æ¯”ç»´åº¦ã€‘
${comparison_aspects}

ã€ç”¨æˆ·é—®é¢˜ã€‘
${query}

ã€å¯¹æ¯”åˆ†æã€‘
è¯·ä»ä»¥ä¸Šç»´åº¦è¿›è¡Œå¯¹æ¯”ï¼Œå¹¶æ€»ç»“ä¸»è¦å·®å¼‚ï¼š"""
        else:
            return """Please compare and analyze ${comparison_target} based on the following references.

ã€Referencesã€‘
${context}

ã€Comparison Aspectsã€‘
${comparison_aspects}

ã€User Questionã€‘
${query}

ã€Comparative Analysisã€‘
Please compare from the above aspects and summarize key differences:"""

    @property
    def required_variables(self) -> List[str]:
        return ['context', 'query', 'comparison_target']

    @property
    def optional_variables(self) -> Dict:
        return {
            'comparison_aspects': 'é€‚ç”¨èŒƒå›´ã€æŠ€æœ¯æŒ‡æ ‡ã€è®¡ç®—æ–¹æ³•ã€æ³¨æ„äº‹é¡¹'
        }


class ExplanationPrompt(BasePrompt):
    """
    è§„èŒƒè§£è¯»Prompt

    ç”¨äºè¯¦ç»†è§£é‡Šè§„èŒƒæ¡æ–‡
    """

    @property
    def template(self) -> str:
        if self.language == 'zh':
            return """è¯·è¯¦ç»†è§£è¯»ä»¥ä¸‹è§„èŒƒæ¡æ–‡ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£å…¶å«ä¹‰å’Œåº”ç”¨ã€‚

ã€è§„èŒƒæ¡æ–‡ã€‘
${context}

ã€è§£è¯»è¦æ±‚ã€‘
1. æ¡æ–‡èƒŒæ™¯ï¼šè¯´æ˜è¯¥æ¡æ–‡çš„åˆ¶å®šèƒŒæ™¯å’Œç›®çš„
2. å…³é”®æœ¯è¯­ï¼šè§£é‡Šæ¡æ–‡ä¸­çš„ä¸“ä¸šæœ¯è¯­
3. é€‚ç”¨åœºæ™¯ï¼šè¯´æ˜è¯¥æ¡æ–‡çš„é€‚ç”¨èŒƒå›´
4. è®¡ç®—æ–¹æ³•ï¼šå¦‚æ¶‰åŠè®¡ç®—ï¼Œç»™å‡ºå…¬å¼å’Œç¤ºä¾‹
5. æ³¨æ„äº‹é¡¹ï¼šæé†’å®é™…åº”ç”¨ä¸­çš„è¦ç‚¹

ã€ç”¨æˆ·é—®é¢˜ã€‘
${query}

ã€è¯¦ç»†è§£è¯»ã€‘"""
        else:
            return """Please provide a detailed explanation of the following regulatory clause.

ã€Regulatory Clauseã€‘
${context}

ã€Explanation Requirementsã€‘
1. Background: Explain background and purpose
2. Key Terms: Define technical terminology
3. Applicable Scenarios: Describe scope of application
4. Calculation Methods: Provide formulas and examples if applicable
5. Important Notes: Highlight key points for practical application

ã€User Questionã€‘
${query}

ã€Detailed Explanationã€‘"""

    @property
    def required_variables(self) -> List[str]:
        return ['context', 'query']


class QAPromptFactory:
    """
    é—®ç­”Promptå·¥å‚

    æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„Promptæ¨¡æ¿
    """

    @staticmethod
    def create_prompt(
            prompt_type: str,
            language: str = 'zh'
    ) -> BasePrompt:
        """
        åˆ›å»ºPromptå®ä¾‹

        å‚æ•°ï¼š
            prompt_type: Promptç±»å‹
                - 'rag': æ ‡å‡†RAGé—®ç­”
                - 'citation': å¼•ç”¨å¼å›ç­”
                - 'comparison': å¯¹æ¯”åˆ†æ
                - 'explanation': è§„èŒƒè§£è¯»
            language: è¯­è¨€

        è¿”å›ï¼š
            Promptå®ä¾‹
        """
        prompt_map = {
            'rag': RAGPrompt,
            'citation': CitationPrompt,
            'comparison': ComparisonPrompt,
            'explanation': ExplanationPrompt
        }

        if prompt_type not in prompt_map:
            raise ValueError(
                f"ä¸æ”¯æŒçš„Promptç±»å‹: {prompt_type}. "
                f"å¯é€‰: {list(prompt_map.keys())}"
            )

        return prompt_map[prompt_type](language=language)

    @staticmethod
    def build_rag_prompt(
            query: str,
            contexts: List[Dict],
            language: str = 'zh',
            max_context_length: int = 3000,
            include_metadata: bool = True
    ) -> str:
        """
        æ„å»ºRAGé—®ç­”Prompt

        å‚æ•°ï¼š
            query: ç”¨æˆ·é—®é¢˜
            contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
                [
                    {
                        'text': 'å†…å®¹',
                        'metadata': {'source': 'æ¥æº', 'score': 0.9}
                    },
                    ...
                ]
            language: è¯­è¨€
            max_context_length: ä¸Šä¸‹æ–‡æœ€å¤§é•¿åº¦
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®

        è¿”å›ï¼š
            å®Œæ•´çš„Prompt
        """
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context_parts = []
        current_length = 0

        for idx, ctx in enumerate(contexts, 1):
            text = ctx.get('text', '')
            metadata = ctx.get('metadata', {})

            # æ£€æŸ¥é•¿åº¦
            if current_length + len(text) > max_context_length:
                break

            # æ ¼å¼åŒ–å•ä¸ªä¸Šä¸‹æ–‡
            if language == 'zh':
                if include_metadata:
                    source = metadata.get('source', f'æ–‡æ¡£{idx}')
                    score = metadata.get('score', 0)
                    context_part = f"ã€æ¥æº{idx}ï¼š{source} | ç›¸å…³åº¦ï¼š{score:.2f}ã€‘\n{text}"
                else:
                    context_part = f"ã€ç‰‡æ®µ{idx}ã€‘\n{text}"
            else:
                if include_metadata:
                    source = metadata.get('source', f'Document{idx}')
                    score = metadata.get('score', 0)
                    context_part = f"ã€Source{idx}: {source} | Relevance: {score:.2f}ã€‘\n{text}"
                else:
                    context_part = f"ã€Snippet{idx}ã€‘\n{text}"

            context_parts.append(context_part)
            current_length += len(text)

        # ç»„åˆä¸Šä¸‹æ–‡
        context = '\n\n'.join(context_parts)

        # åˆ›å»ºPrompt
        prompt = RAGPrompt(language=language)
        return prompt.format(context=context, query=query)


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from services.llm.prompt.qa_prompt import (
    RAGPrompt,
    CitationPrompt,
    QAPromptFactory
)

# 1. ä½¿ç”¨RAG Prompt
rag_prompt = RAGPrompt(language='zh')

context = '''
GB50009-2012 å»ºç­‘ç»“æ„è·è½½è§„èŒƒ
ç¬¬4.1.1æ¡ï¼šæ°‘ç”¨å»ºç­‘æ¥¼é¢å‡å¸ƒæ´»è·è½½çš„æ ‡å‡†å€¼åŠå…¶ç»„åˆå€¼ã€
é¢‘é‡å€¼å’Œå‡†æ°¸ä¹…å€¼ç³»æ•°ï¼Œåº”æŒ‰è¡¨4.1.1é‡‡ç”¨ã€‚
'''

query = "åŠå…¬å®¤æ¥¼é¢æ´»è·è½½æ ‡å‡†å€¼æ˜¯å¤šå°‘ï¼Ÿ"

prompt = rag_prompt.format(context=context, query=query)
print(prompt)


# 2. ä½¿ç”¨å¼•ç”¨å¼Prompt
citation_prompt = CitationPrompt(language='zh')
prompt = citation_prompt.format(context=context, query=query)


# 3. ä½¿ç”¨Promptå·¥å‚
prompt = QAPromptFactory.create_prompt('rag', language='zh')


# 4. æ„å»ºå®Œæ•´çš„RAG Promptï¼ˆæ¨èï¼‰
contexts = [
    {
        'text': 'GB50009-2012è§„å®šï¼ŒåŠå…¬å®¤æ¥¼é¢æ´»è·è½½æ ‡å‡†å€¼ä¸º2.0kN/mÂ²',
        'metadata': {
            'source': 'GB50009-2012',
            'score': 0.95,
            'doc_id': 'doc_001'
        }
    },
    {
        'text': 'å¯¹äºé‡è¦åŠå…¬å®¤ï¼Œæ´»è·è½½å¯é€‚å½“æé«˜',
        'metadata': {
            'source': 'GB50009-2012æ¡æ–‡è¯´æ˜',
            'score': 0.82,
            'doc_id': 'doc_002'
        }
    }
]

query = "åŠå…¬å®¤æ¥¼é¢æ´»è·è½½å¦‚ä½•å–å€¼ï¼Ÿ"

final_prompt = QAPromptFactory.build_rag_prompt(
    query=query,
    contexts=contexts,
    language='zh',
    max_context_length=3000,
    include_metadata=True
)

print(final_prompt)


# 5. å¯¹æ¯”åˆ†æPrompt
comparison = ComparisonPrompt(language='zh')
prompt = comparison.format(
    context='æ–°æ—§è§„èŒƒå†…å®¹...',
    query='GB50009-2012ä¸GB50009-2001æœ‰ä½•å·®å¼‚ï¼Ÿ',
    comparison_target='æ–°æ—§è·è½½è§„èŒƒ',
    comparison_aspects='è·è½½åˆ†ç±»ã€å–å€¼æ–¹æ³•ã€ç»„åˆåŸåˆ™'
)
"""