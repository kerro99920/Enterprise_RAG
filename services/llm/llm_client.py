"""
========================================
LLM å®¢æˆ·ç«¯
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£
- æ”¯æŒ OpenAI å…¼å®¹ API
- æ”¯æŒå¤šç§æ¨¡å‹åˆ‡æ¢

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åŒæ­¥/å¼‚æ­¥è°ƒç”¨
2. æµå¼è¾“å‡º
3. é”™è¯¯é‡è¯•
4. Token ç»Ÿè®¡

========================================
"""

import os
import time
import asyncio
from typing import List, Dict, Optional, Generator, AsyncGenerator, Union

import httpx
from openai import OpenAI, AsyncOpenAI
from loguru import logger

from core.config import settings


class LLMClient:
    """
    LLM å®¢æˆ·ç«¯

    ğŸ”§ æ”¯æŒçš„ APIï¼š
    - OpenAI API
    - OpenAI å…¼å®¹ APIï¼ˆå¦‚ vLLMã€Ollamaã€é€šä¹‰åƒé—®ç­‰ï¼‰

    ğŸ’¡ ç‰¹æ€§ï¼š
    - åŒæ­¥/å¼‚æ­¥è°ƒç”¨
    - æµå¼è¾“å‡º
    - è‡ªåŠ¨é‡è¯•
    - ä½¿ç”¨ç»Ÿè®¡
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        timeout: int = 60,
        max_retries: int = 3
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        å‚æ•°ï¼š
            api_key: API å¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            api_base: API åŸºç¡€ URLï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆ0-1ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        # API é…ç½®
        self.api_key = api_key or os.getenv("OPENAI_API_KEY", "sk-placeholder")
        self.api_base = api_base or os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        self.model = model or os.getenv("LLM_MODEL", "gpt-3.5-turbo")

        # ç”Ÿæˆå‚æ•°
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._sync_client = None
        self._async_client = None

        # ä½¿ç”¨ç»Ÿè®¡
        self.total_requests = 0
        self.total_tokens = 0
        self.total_errors = 0

        logger.info(
            f"LLM å®¢æˆ·ç«¯åˆå§‹åŒ– | "
            f"æ¨¡å‹: {self.model} | "
            f"API: {self.api_base}"
        )

    @property
    def sync_client(self) -> OpenAI:
        """è·å–åŒæ­¥å®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._sync_client is None:
            self._sync_client = OpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                timeout=self.timeout,
                max_retries=self.max_retries
            )
        return self._sync_client

    @property
    def async_client(self) -> AsyncOpenAI:
        """è·å–å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._async_client is None:
            self._async_client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                timeout=self.timeout,
                max_retries=self.max_retries
            )
        return self._async_client

    def chat(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        åŒæ­¥èŠå¤©è°ƒç”¨

        å‚æ•°ï¼š
            messages: æ¶ˆæ¯åˆ—è¡¨
                [
                    {"role": "system", "content": "..."},
                    {"role": "user", "content": "..."}
                ]
            model: è¦†ç›–é»˜è®¤æ¨¡å‹
            temperature: è¦†ç›–é»˜è®¤æ¸©åº¦
            max_tokens: è¦†ç›–é»˜è®¤æœ€å¤§ token
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™ API

        è¿”å›ï¼š
            str: ç”Ÿæˆçš„å›å¤å†…å®¹
        """
        try:
            self.total_requests += 1

            response = self.sync_client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens or self.max_tokens,
                **kwargs
            )

            # ç»Ÿè®¡ token
            if hasattr(response, 'usage') and response.usage:
                self.total_tokens += response.usage.total_tokens

            content = response.choices[0].message.content

            logger.debug(
                f"LLM è°ƒç”¨æˆåŠŸ | "
                f"æ¨¡å‹: {model or self.model} | "
                f"å›å¤é•¿åº¦: {len(content)}"
            )

            return content

        except Exception as e:
            self.total_errors += 1
            logger.error(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            raise

    async def chat_async(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        å¼‚æ­¥èŠå¤©è°ƒç”¨

        å‚æ•°ä¸ chat() ç›¸åŒ
        """
        try:
            self.total_requests += 1

            response = await self.async_client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens or self.max_tokens,
                **kwargs
            )

            # ç»Ÿè®¡ token
            if hasattr(response, 'usage') and response.usage:
                self.total_tokens += response.usage.total_tokens

            content = response.choices[0].message.content

            logger.debug(
                f"LLM å¼‚æ­¥è°ƒç”¨æˆåŠŸ | "
                f"æ¨¡å‹: {model or self.model} | "
                f"å›å¤é•¿åº¦: {len(content)}"
            )

            return content

        except Exception as e:
            self.total_errors += 1
            logger.error(f"LLM å¼‚æ­¥è°ƒç”¨å¤±è´¥: {e}")
            raise

    def chat_stream(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Generator[str, None, None]:
        """
        æµå¼èŠå¤©è°ƒç”¨ï¼ˆåŒæ­¥ï¼‰

        å‚æ•°ä¸ chat() ç›¸åŒ

        è¿”å›ï¼š
            Generator[str]: é€å­—ç¬¦/é€ token çš„ç”Ÿæˆå™¨
        """
        try:
            self.total_requests += 1

            stream = self.sync_client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens or self.max_tokens,
                stream=True,
                **kwargs
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            self.total_errors += 1
            logger.error(f"LLM æµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise

    async def chat_stream_async(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼èŠå¤©è°ƒç”¨ï¼ˆå¼‚æ­¥ï¼‰

        å‚æ•°ä¸ chat() ç›¸åŒ

        è¿”å›ï¼š
            AsyncGenerator[str]: é€å­—ç¬¦/é€ token çš„å¼‚æ­¥ç”Ÿæˆå™¨
        """
        try:
            self.total_requests += 1

            stream = await self.async_client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens or self.max_tokens,
                stream=True,
                **kwargs
            )

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            self.total_errors += 1
            logger.error(f"LLM å¼‚æ­¥æµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise

    def complete(
        self,
        prompt: str,
        model: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ç®€å•çš„æ–‡æœ¬è¡¥å…¨ï¼ˆå°† prompt åŒ…è£…ä¸º user messageï¼‰

        å‚æ•°ï¼š
            prompt: æç¤ºæ–‡æœ¬
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°

        è¿”å›ï¼š
            str: ç”Ÿæˆçš„å›å¤
        """
        messages = [{"role": "user", "content": prompt}]
        return self.chat(messages, model=model, **kwargs)

    async def complete_async(
        self,
        prompt: str,
        model: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ç®€å•çš„æ–‡æœ¬è¡¥å…¨ï¼ˆå¼‚æ­¥ï¼‰

        å‚æ•°ä¸ complete() ç›¸åŒ
        """
        messages = [{"role": "user", "content": prompt}]
        return await self.chat_async(messages, model=model, **kwargs)

    def get_embedding(
        self,
        text: Union[str, List[str]],
        model: str = "text-embedding-ada-002"
    ) -> Union[List[float], List[List[float]]]:
        """
        è·å–æ–‡æœ¬åµŒå…¥å‘é‡

        å‚æ•°ï¼š
            text: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            model: åµŒå…¥æ¨¡å‹åç§°

        è¿”å›ï¼š
            å•ä¸ªå‘é‡æˆ–å‘é‡åˆ—è¡¨
        """
        try:
            response = self.sync_client.embeddings.create(
                model=model,
                input=text
            )

            if isinstance(text, str):
                return response.data[0].embedding
            else:
                return [item.embedding for item in response.data]

        except Exception as e:
            logger.error(f"è·å– Embedding å¤±è´¥: {e}")
            raise

    def get_stats(self) -> Dict:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        return {
            'total_requests': self.total_requests,
            'total_tokens': self.total_tokens,
            'total_errors': self.total_errors,
            'model': self.model,
            'api_base': self.api_base
        }

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.total_requests = 0
        self.total_tokens = 0
        self.total_errors = 0

    async def ping(self) -> bool:
        """
        æµ‹è¯• API è¿æ¥

        è¿”å›ï¼š
            bool: è¿æ¥æ˜¯å¦æ­£å¸¸
        """
        try:
            # å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚æµ‹è¯•è¿æ¥
            await self.chat_async(
                messages=[{"role": "user", "content": "Hi"}],
                max_tokens=5
            )
            return True
        except Exception as e:
            logger.error(f"API è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False


# =========================================
# å·¥å‚å‡½æ•°
# =========================================

def create_llm_client(
    provider: str = "openai",
    **kwargs
) -> LLMClient:
    """
    åˆ›å»º LLM å®¢æˆ·ç«¯çš„å·¥å‚å‡½æ•°

    å‚æ•°ï¼š
        provider: æä¾›å•†
            - "openai": OpenAI API
            - "qwen": é€šä¹‰åƒé—®
            - "glm": æ™ºè°± GLM
            - "ollama": Ollama æœ¬åœ°éƒ¨ç½²
            - "vllm": vLLM éƒ¨ç½²
        **kwargs: ä¼ é€’ç»™ LLMClient çš„å‚æ•°

    è¿”å›ï¼š
        LLMClient: é…ç½®å¥½çš„å®¢æˆ·ç«¯å®ä¾‹
    """
    configs = {
        "openai": {
            "api_base": "https://api.openai.com/v1",
            "model": "gpt-3.5-turbo"
        },
        "qwen": {
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "model": "qwen-plus"
        },
        "glm": {
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "model": "glm-4"
        },
        "ollama": {
            "api_base": "http://localhost:11434/v1",
            "model": "llama2"
        },
        "vllm": {
            "api_base": "http://localhost:8000/v1",
            "model": "Qwen/Qwen2-7B-Instruct"
        }
    }

    if provider not in configs:
        logger.warning(f"æœªçŸ¥çš„æä¾›å•†: {provider}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = {}
    else:
        config = configs[provider]

    # åˆå¹¶é…ç½®
    config.update(kwargs)

    return LLMClient(**config)


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
# 1. åŸºç¡€ä½¿ç”¨
from services.llm.llm_client import LLMClient

client = LLMClient(
    api_key="your-api-key",
    api_base="https://api.openai.com/v1",
    model="gpt-3.5-turbo"
)

# åŒæ­¥è°ƒç”¨
response = client.chat([
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯RAGï¼Ÿ"}
])
print(response)


# 2. å¼‚æ­¥è°ƒç”¨
import asyncio

async def main():
    response = await client.chat_async([
        {"role": "user", "content": "è§£é‡Šä¸€ä¸‹å‘é‡æ£€ç´¢"}
    ])
    print(response)

asyncio.run(main())


# 3. æµå¼è¾“å‡º
print("æµå¼è¾“å‡º: ", end="", flush=True)
for chunk in client.chat_stream([
    {"role": "user", "content": "å†™ä¸€é¦–å…³äºAIçš„è¯—"}
]):
    print(chunk, end="", flush=True)
print()


# 4. å¼‚æ­¥æµå¼è¾“å‡º
async def stream_demo():
    async for chunk in client.chat_stream_async([
        {"role": "user", "content": "è®²ä¸€ä¸ªç¬‘è¯"}
    ]):
        print(chunk, end="", flush=True)

asyncio.run(stream_demo())


# 5. ä½¿ç”¨å·¥å‚å‡½æ•°
from services.llm.llm_client import create_llm_client

# åˆ›å»ºé€šä¹‰åƒé—®å®¢æˆ·ç«¯
qwen_client = create_llm_client(
    provider="qwen",
    api_key="your-qwen-api-key"
)

# åˆ›å»º Ollama å®¢æˆ·ç«¯
ollama_client = create_llm_client(
    provider="ollama",
    model="llama2"
)


# 6. ç®€å•è¡¥å…¨
answer = client.complete("è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
print(answer)


# 7. æŸ¥çœ‹ç»Ÿè®¡
stats = client.get_stats()
print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
print(f"æ€» Token æ•°: {stats['total_tokens']}")


# 8. æµ‹è¯•è¿æ¥
is_connected = await client.ping()
print(f"API è¿æ¥: {'æ­£å¸¸' if is_connected else 'å¼‚å¸¸'}")
"""
