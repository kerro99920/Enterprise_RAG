"""
========================================
LLMå®¢æˆ·ç«¯
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- ç»Ÿä¸€çš„LLM APIè°ƒç”¨æ¥å£
- æ”¯æŒå¤šç§LLMæœåŠ¡
- é”™è¯¯é‡è¯•å’Œæµå¼è¾“å‡º

ğŸ¯ æ”¯æŒçš„LLMï¼š
1. OpenAI APIå…¼å®¹æ¥å£ï¼ˆGPTã€Qwenã€GLMç­‰ï¼‰
2. æœ¬åœ°æ¨¡å‹ï¼ˆé€šè¿‡vLLMã€Ollamaç­‰ï¼‰
3. è‡ªå®šä¹‰API

========================================
"""

import time
from typing import List, Dict, Optional, Generator, Union
import json

from openai import OpenAI
from loguru import logger


class LLMClient:
    """
    LLMå®¢æˆ·ç«¯

    ğŸ”§ åŠŸèƒ½ï¼š
    - ç»Ÿä¸€APIè°ƒç”¨
    - æ”¯æŒæµå¼è¾“å‡º
    - è‡ªåŠ¨é‡è¯•
    - é”™è¯¯å¤„ç†

    ğŸ’¡ å…¼å®¹æ€§ï¼š
    - OpenAI API
    - Azure OpenAI
    - é˜¿é‡Œäº‘é€šä¹‰åƒé—®
    - æ™ºè°±GLM
    - æœ¬åœ°vLLM/Ollama
    """

    def __init__(
            self,
            api_base: str = "http://localhost:8000/v1",
            api_key: str = "EMPTY",
            model: str = "qwen-plus",
            temperature: float = 0.7,
            max_tokens: int = 2000,
            timeout: int = 60,
            max_retries: int = 3
    ):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        å‚æ•°ï¼š
            api_base: APIåœ°å€
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-2ï¼Œè¶Šä½è¶Šç¡®å®šï¼‰
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.api_base = api_base
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=api_key,
            base_url=api_base,
            timeout=timeout
        )

        logger.info(
            f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ– | "
            f"æ¨¡å‹: {model} | "
            f"API: {api_base}"
        )

    def chat(
            self,
            messages: List[Dict[str, str]],
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
            stream: bool = False,
            **kwargs
    ) -> Union[str, Generator[str, None, None]]:
        """
        å¯¹è¯è¡¥å…¨

        å‚æ•°ï¼š
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
                [
                    {"role": "system", "content": "ç³»ç»Ÿæç¤º"},
                    {"role": "user", "content": "ç”¨æˆ·æ¶ˆæ¯"},
                    {"role": "assistant", "content": "åŠ©æ‰‹å›å¤"}
                ]
            temperature: æ¸©åº¦ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
            max_tokens: æœ€å¤§tokenï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–APIå‚æ•°

        è¿”å›ï¼š
            - stream=False: å®Œæ•´å›å¤æ–‡æœ¬
            - stream=True: æ–‡æœ¬ç”Ÿæˆå™¨
        """
        # ä½¿ç”¨é»˜è®¤å€¼
        if temperature is None:
            temperature = self.temperature
        if max_tokens is None:
            max_tokens = self.max_tokens

        logger.debug(
            f"è°ƒç”¨LLM | æ¨¡å‹: {self.model} | "
            f"æ¶ˆæ¯æ•°: {len(messages)} | "
            f"æµå¼: {stream}"
        )

        # å¸¦é‡è¯•çš„APIè°ƒç”¨
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stream=stream,
                    **kwargs
                )

                if stream:
                    return self._stream_response(response)
                else:
                    content = response.choices[0].message.content
                    logger.debug(f"LLMå“åº”é•¿åº¦: {len(content)}")
                    return content

            except Exception as e:
                logger.warning(
                    f"LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}"
                )

                if attempt == self.max_retries - 1:
                    logger.error(f"LLMè°ƒç”¨æœ€ç»ˆå¤±è´¥: {e}")
                    raise

                # æŒ‡æ•°é€€é¿
                time.sleep(2 ** attempt)

    def _stream_response(
            self,
            response
    ) -> Generator[str, None, None]:
        """
        å¤„ç†æµå¼å“åº”

        ç”Ÿæˆå™¨ï¼Œé€æ­¥yieldæ–‡æœ¬ç‰‡æ®µ
        """
        try:
            for chunk in response:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            logger.error(f"æµå¼å“åº”å¤„ç†å¤±è´¥: {e}")
            raise

    def generate(
            self,
            prompt: str,
            system_prompt: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
            stream: bool = False,
            **kwargs
    ) -> Union[str, Generator[str, None, None]]:
        """
        ç®€åŒ–çš„ç”Ÿæˆæ¥å£

        å‚æ•°ï¼š
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§token
            stream: æ˜¯å¦æµå¼
            **kwargs: å…¶ä»–å‚æ•°

        è¿”å›ï¼š
            ç”Ÿæˆçš„æ–‡æœ¬æˆ–æ–‡æœ¬ç”Ÿæˆå™¨
        """
        # æ„å»ºæ¶ˆæ¯
        messages = []

        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })

        messages.append({
            "role": "user",
            "content": prompt
        })

        return self.chat(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=stream,
            **kwargs
        )

    def batch_generate(
            self,
            prompts: List[str],
            system_prompt: Optional[str] = None,
            **kwargs
    ) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆ

        å‚æ•°ï¼š
            prompts: æç¤ºåˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤º
            **kwargs: å…¶ä»–å‚æ•°

        è¿”å›ï¼š
            ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        results = []

        logger.info(f"æ‰¹é‡ç”Ÿæˆ | æ•°é‡: {len(prompts)}")

        for idx, prompt in enumerate(prompts, 1):
            logger.debug(f"ç”Ÿæˆ {idx}/{len(prompts)}")

            try:
                result = self.generate(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    **kwargs
                )
                results.append(result)
            except Exception as e:
                logger.error(f"æ‰¹é‡ç”Ÿæˆå¤±è´¥ ({idx}/{len(prompts)}): {e}")
                results.append("")  # å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²

        logger.info(f"æ‰¹é‡ç”Ÿæˆå®Œæˆ | æˆåŠŸ: {sum(1 for r in results if r)}/{len(prompts)}")

        return results

    def count_tokens(self, text: str) -> int:
        """
        ä¼°ç®—tokenæ•°é‡

        ç®€å•ä¼°ç®—ï¼šä¸­æ–‡1å­—â‰ˆ1tokenï¼Œè‹±æ–‡1è¯â‰ˆ1.3token

        å‚æ•°ï¼š
            text: æ–‡æœ¬

        è¿”å›ï¼š
            ä¼°ç®—çš„tokenæ•°
        """
        import re

        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))

        # ç»Ÿè®¡è‹±æ–‡å•è¯
        english_words = len(re.findall(r'[a-zA-Z]+', text))

        # ä¼°ç®—
        tokens = chinese_chars + int(english_words * 1.3)

        return tokens

    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹é…ç½®ä¿¡æ¯"""
        return {
            'model': self.model,
            'api_base': self.api_base,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'timeout': self.timeout
        }


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from services.llm.llm_client import LLMClient

# 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
client = LLMClient(
    api_base="http://localhost:8000/v1",
    api_key="your_api_key",
    model="qwen-plus",
    temperature=0.7,
    max_tokens=2000
)

# 2. ç®€å•ç”Ÿæˆ
response = client.generate(
    prompt="ä»€ä¹ˆæ˜¯å»ºç­‘ç»“æ„è·è½½ï¼Ÿ",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥ç¨‹å¸ˆã€‚"
)
print(response)


# 3. å¯¹è¯æ¨¡å¼
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å»ºç­‘å·¥ç¨‹å¸ˆã€‚"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯æ¥¼é¢è·è½½ï¼Ÿ"},
    {"role": "assistant", "content": "æ¥¼é¢è·è½½æ˜¯æŒ‡ä½œç”¨åœ¨æ¥¼æ¿ä¸Šçš„..."},
    {"role": "user", "content": "é‚£æ´»è·è½½å‘¢ï¼Ÿ"}
]

response = client.chat(messages=messages)
print(response)


# 4. æµå¼è¾“å‡º
print("æµå¼è¾“å‡ºï¼š", end="", flush=True)
for chunk in client.generate(
    prompt="è¯·è¯¦ç»†ä»‹ç»å»ºç­‘è·è½½è§„èŒƒ",
    stream=True
):
    print(chunk, end="", flush=True)
print()


# 5. æ‰¹é‡ç”Ÿæˆ
prompts = [
    "ä»€ä¹ˆæ˜¯æ’è·è½½ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯æ´»è·è½½ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯é£è·è½½ï¼Ÿ"
]

results = client.batch_generate(
    prompts=prompts,
    system_prompt="ä½ æ˜¯å·¥ç¨‹å¸ˆï¼Œç®€æ´å›ç­”ã€‚"
)

for prompt, result in zip(prompts, results):
    print(f"Q: {prompt}")
    print(f"A: {result}\n")


# 6. Tokenè®¡æ•°
text = "å»ºç­‘ç»“æ„è·è½½è§„èŒƒGB50009-2012"
tokens = client.count_tokens(text)
print(f"Tokenæ•°: {tokens}")


# 7. æŸ¥çœ‹é…ç½®
info = client.get_model_info()
print(f"æ¨¡å‹é…ç½®: {info}")


# 8. ä½¿ç”¨ä¸åŒçš„LLM
# OpenAI GPT
openai_client = LLMClient(
    api_base="https://api.openai.com/v1",
    api_key="sk-xxx",
    model="gpt-4"
)

# æ™ºè°±GLM
glm_client = LLMClient(
    api_base="https://open.bigmodel.cn/api/paas/v4",
    api_key="your_glm_key",
    model="glm-4"
)

# æœ¬åœ°Ollama
ollama_client = LLMClient(
    api_base="http://localhost:11434/v1",
    api_key="ollama",
    model="qwen2:7b"
)
"""