"""
========================================
BM25å…³é”®è¯æ£€ç´¢å™¨
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- åŸºäºBM25ç®—æ³•çš„å…³é”®è¯æ£€ç´¢
- ç¨€ç–æ£€ç´¢ï¼Œäº’è¡¥å‘é‡æ£€ç´¢
- é€‚åˆç²¾ç¡®åŒ¹é…åœºæ™¯

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. BM25ç´¢å¼•æ„å»º
2. å…³é”®è¯æ£€ç´¢
3. å¢é‡ç´¢å¼•æ›´æ–°
4. ç»“æœæ’åº

========================================
"""

import pickle
from typing import List, Dict, Tuple, Optional
from pathlib import Path

import numpy as np
from rank_bm25 import BM25Okapi
from loguru import logger

from services.retrieval.text_utils import TextProcessor


class BM25Retriever:
    """
    BM25æ£€ç´¢å™¨

    ğŸ”§ æŠ€æœ¯ç‰¹ç‚¹ï¼š
    - BM25Okapiç®—æ³•ï¼ˆæ”¹è¿›ç‰ˆBM25ï¼‰
    - ä¸­æ–‡åˆ†è¯æ”¯æŒ
    - æ”¯æŒç´¢å¼•æŒä¹…åŒ–

    ğŸ’¡ é€‚ç”¨åœºæ™¯ï¼š
    - å…³é”®è¯ç²¾ç¡®åŒ¹é…
    - ä¸“ä¸šæœ¯è¯­æ£€ç´¢
    - è§„èŒƒæ ‡å‡†æŸ¥è¯¢
    """

    def __init__(
            self,
            text_processor: Optional[TextProcessor] = None,
            k1: float = 1.5,
            b: float = 0.75
    ):
        """
        åˆå§‹åŒ–BM25æ£€ç´¢å™¨

        å‚æ•°ï¼š
            text_processor: æ–‡æœ¬å¤„ç†å™¨å®ä¾‹
            k1: BM25å‚æ•°k1ï¼ˆè¯é¢‘é¥±å’Œåº¦ï¼Œæ¨è1.2-2.0ï¼‰
            b: BM25å‚æ•°bï¼ˆæ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–ï¼Œæ¨è0.75ï¼‰
        """
        self.text_processor = text_processor or TextProcessor()
        self.k1 = k1
        self.b = b

        # BM25æ¨¡å‹
        self.bm25_model = None

        # æ–‡æ¡£æ•°æ®
        self.documents = []  # åŸå§‹æ–‡æ¡£
        self.tokenized_docs = []  # åˆ†è¯åçš„æ–‡æ¡£
        self.doc_ids = []  # æ–‡æ¡£ID

        logger.info(f"BM25æ£€ç´¢å™¨åˆå§‹åŒ– | k1={k1}, b={b}")

    def build_index(
            self,
            documents: List[Dict],
            text_key: str = 'text',
            id_key: str = 'id'
    ):
        """
        æ„å»ºBM25ç´¢å¼•

        å‚æ•°ï¼š
            documents: æ–‡æ¡£åˆ—è¡¨
                [
                    {'id': 'doc1', 'text': 'æ–‡æ¡£å†…å®¹', ...},
                    ...
                ]
            text_key: æ–‡æœ¬å­—æ®µçš„é”®å
            id_key: IDå­—æ®µçš„é”®å
        """
        logger.info(f"å¼€å§‹æ„å»ºBM25ç´¢å¼• | æ–‡æ¡£æ•°: {len(documents)}")

        # é‡ç½®æ•°æ®
        self.documents = []
        self.tokenized_docs = []
        self.doc_ids = []

        # å¤„ç†æ–‡æ¡£
        for idx, doc in enumerate(documents):
            # è·å–æ–‡æœ¬
            text = doc.get(text_key, '')
            if not text or not text.strip():
                logger.warning(f"æ–‡æ¡£{idx}æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # åˆ†è¯
            tokens = self.text_processor.tokenize(text, mode='search')
            if not tokens:
                logger.warning(f"æ–‡æ¡£{idx}åˆ†è¯åä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # ä¿å­˜
            self.documents.append(doc)
            self.tokenized_docs.append(tokens)

            # è·å–IDï¼ˆå¦‚æœæ²¡æœ‰IDï¼Œä½¿ç”¨ç´¢å¼•ï¼‰
            doc_id = doc.get(id_key, f"doc_{idx}")
            self.doc_ids.append(doc_id)

        # æ„å»ºBM25æ¨¡å‹
        if self.tokenized_docs:
            self.bm25_model = BM25Okapi(
                self.tokenized_docs,
                k1=self.k1,
                b=self.b
            )

            logger.info(
                f"BM25ç´¢å¼•æ„å»ºå®Œæˆ | "
                f"æœ‰æ•ˆæ–‡æ¡£: {len(self.tokenized_docs)} | "
                f"å¹³å‡è¯æ•°: {np.mean([len(d) for d in self.tokenized_docs]):.1f}"
            )
        else:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œç´¢å¼•ä¸ºç©º")

    def search(
            self,
            query: str,
            top_k: int = 10,
            return_scores: bool = True
    ) -> List[Dict]:
        """
        æ£€ç´¢æ–‡æ¡£

        å‚æ•°ï¼š
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            return_scores: æ˜¯å¦è¿”å›åˆ†æ•°

        è¿”å›ï¼š
            æ£€ç´¢ç»“æœåˆ—è¡¨
            [
                {
                    'document': Dict,  # åŸå§‹æ–‡æ¡£
                    'score': float,    # BM25åˆ†æ•°
                    'rank': int,       # æ’å
                    'doc_id': str      # æ–‡æ¡£ID
                },
                ...
            ]
        """
        if not self.bm25_model:
            logger.warning("BM25ç´¢å¼•æœªæ„å»ºï¼Œè¿”å›ç©ºç»“æœ")
            return []

        if not query or not query.strip():
            logger.warning("æŸ¥è¯¢æ–‡æœ¬ä¸ºç©º")
            return []

        logger.debug(f"BM25æ£€ç´¢ | æŸ¥è¯¢: {query[:50]}... | top_k: {top_k}")

        # æŸ¥è¯¢åˆ†è¯
        query_tokens = self.text_processor.tokenize(query, mode='search')
        if not query_tokens:
            logger.warning("æŸ¥è¯¢åˆ†è¯åä¸ºç©º")
            return []

        # è®¡ç®—BM25åˆ†æ•°
        scores = self.bm25_model.get_scores(query_tokens)

        # è·å–Top-Kç´¢å¼•
        top_k = min(top_k, len(scores))
        top_indices = np.argsort(scores)[::-1][:top_k]

        # æ„å»ºç»“æœ
        results = []
        for rank, idx in enumerate(top_indices, 1):
            score = float(scores[idx])

            # è¿‡æ»¤0åˆ†ç»“æœ
            if score <= 0:
                break

            result = {
                'document': self.documents[idx],
                'doc_id': self.doc_ids[idx],
                'rank': rank
            }

            if return_scores:
                result['score'] = score

            results.append(result)

        logger.debug(f"BM25æ£€ç´¢å®Œæˆ | è¿”å›: {len(results)} ä¸ªç»“æœ")

        return results

    def add_documents(
            self,
            new_documents: List[Dict],
            text_key: str = 'text',
            id_key: str = 'id'
    ):
        """
        å¢é‡æ·»åŠ æ–‡æ¡£ï¼ˆé‡å»ºç´¢å¼•ï¼‰

        å‚æ•°ï¼š
            new_documents: æ–°æ–‡æ¡£åˆ—è¡¨
            text_key: æ–‡æœ¬å­—æ®µé”®å
            id_key: IDå­—æ®µé”®å
        """
        logger.info(f"å¢é‡æ·»åŠ æ–‡æ¡£ | æ–°å¢: {len(new_documents)}")

        # åˆå¹¶æ–‡æ¡£
        all_documents = self.documents + new_documents

        # é‡å»ºç´¢å¼•
        self.build_index(all_documents, text_key, id_key)

    def save(self, filepath: str):
        """
        ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶

        å‚æ•°:
            filepath: ä¿å­˜è·¯å¾„
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)

        data = {
            'documents': self.documents,
            'tokenized_docs': self.tokenized_docs,
            'doc_ids': self.doc_ids,
            'k1': self.k1,
            'b': self.b
        }

        with open(filepath, 'wb') as f:
            pickle.dump(data, f)

        logger.info(f"BM25ç´¢å¼•å·²ä¿å­˜: {filepath}")

    def load(self, filepath: str):
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•

        å‚æ•°:
            filepath: æ–‡ä»¶è·¯å¾„
        """
        filepath = Path(filepath)

        if not filepath.exists():
            raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")

        with open(filepath, 'rb') as f:
            data = pickle.load(f)

        self.documents = data['documents']
        self.tokenized_docs = data['tokenized_docs']
        self.doc_ids = data['doc_ids']
        self.k1 = data.get('k1', 1.5)
        self.b = data.get('b', 0.75)

        # é‡å»ºBM25æ¨¡å‹
        if self.tokenized_docs:
            self.bm25_model = BM25Okapi(
                self.tokenized_docs,
                k1=self.k1,
                b=self.b
            )

        logger.info(
            f"BM25ç´¢å¼•å·²åŠ è½½: {filepath} | "
            f"æ–‡æ¡£æ•°: {len(self.documents)}"
        )

    def get_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        if not self.tokenized_docs:
            return {'total_docs': 0}

        token_counts = [len(doc) for doc in self.tokenized_docs]

        return {
            'total_docs': len(self.documents),
            'avg_doc_length': np.mean(token_counts),
            'min_doc_length': np.min(token_counts),
            'max_doc_length': np.max(token_counts),
            'k1': self.k1,
            'b': self.b
        }


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from services.retrieval.bm25_retriever import BM25Retriever

# 1. æ„å»ºç´¢å¼•
documents = [
    {
        'id': 'doc1',
        'text': 'å»ºç­‘ç»“æ„è·è½½è§„èŒƒ GB50009-2012',
        'metadata': {'type': 'regulation'}
    },
    {
        'id': 'doc2',
        'text': 'å»ºç­‘æŠ—éœ‡è®¾è®¡è§„èŒƒ GB50011-2010',
        'metadata': {'type': 'regulation'}
    },
    {
        'id': 'doc3',
        'text': 'æ··å‡åœŸç»“æ„è®¾è®¡è§„èŒƒ GB50010-2010',
        'metadata': {'type': 'regulation'}
    }
]

retriever = BM25Retriever()
retriever.build_index(documents)


# 2. æ£€ç´¢
query = "å»ºç­‘ç»“æ„è·è½½å¦‚ä½•è®¡ç®—ï¼Ÿ"
results = retriever.search(query, top_k=5)

for result in results:
    print(f"æ’å: {result['rank']}")
    print(f"åˆ†æ•°: {result['score']:.4f}")
    print(f"æ–‡æ¡£: {result['document']['text']}")
    print("---")


# 3. ä¿å­˜å’ŒåŠ è½½ç´¢å¼•
retriever.save("data/indexes/bm25_index.pkl")

new_retriever = BM25Retriever()
new_retriever.load("data/indexes/bm25_index.pkl")


# 4. å¢é‡æ·»åŠ æ–‡æ¡£
new_docs = [
    {'id': 'doc4', 'text': 'å»ºç­‘åœ°åŸºåŸºç¡€è®¾è®¡è§„èŒƒ'}
]
retriever.add_documents(new_docs)


# 5. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
stats = retriever.get_stats()
print(f"ç´¢å¼•ç»Ÿè®¡: {stats}")
"""