"""
========================================
RAG Pipeline - æ ¸å¿ƒæµç¨‹ç¼–æ’
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- æ•´åˆæ£€ç´¢ã€é‡æ’ã€LLM è°ƒç”¨çš„å®Œæ•´ RAG æµç¨‹
- æä¾›ç»Ÿä¸€çš„é—®ç­”æ¥å£
- æ”¯æŒå¼‚æ­¥è°ƒç”¨

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æŸ¥è¯¢é¢„å¤„ç†
2. æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + BM25ï¼‰
3. é‡æ’åº
4. LLM ç­”æ¡ˆç”Ÿæˆ
5. ç»“æœåå¤„ç†

========================================
"""

from __future__ import annotations

import asyncio
from typing import Any, Dict, List, Optional, Union
from datetime import datetime

from loguru import logger

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from core.config import settings
from services.retrieval.hybrid.hybrid_retriever import HybridRetriever
from services.retrieval.bm25.bm25_engine import BM25Retriever
from services.retrieval.vector.vector_engine import VectorRetriever
from services.rerank.reranker import Reranker
from services.embedding.embedder import Embedder
from services.embedding.embedding_model import EmbeddingModel
from services.llm.llm_client import LLMClient
from services.llm.prompt.qa_prompt import QAPromptFactory
from services.cache.redis_client import redis_client

# å›¾è°±å¢å¼ºæ£€ç´¢ç»„ä»¶
try:
    from services.retrieval.graph.graph_retriever import GraphRetriever
    from services.retrieval.graph_enhanced_retriever import GraphEnhancedRetriever
    GRAPH_RETRIEVAL_AVAILABLE = True
except ImportError:
    GRAPH_RETRIEVAL_AVAILABLE = False
    logger.warning("å›¾è°±æ£€ç´¢ç»„ä»¶æœªåŠ è½½ï¼Œå›¾è°±å¢å¼ºåŠŸèƒ½ä¸å¯ç”¨")


class RagPipeline:
    """
    RAG æµç¨‹ç¼–æ’å™¨

    ğŸ”§ æ ¸å¿ƒæµç¨‹ï¼š
    1. æŸ¥è¯¢é¢„å¤„ç†ï¼ˆQuery Preprocessingï¼‰
    2. ç¼“å­˜æ£€æŸ¥ï¼ˆCache Checkï¼‰
    3. æ··åˆæ£€ç´¢ï¼ˆHybrid Retrievalï¼‰
    4. é‡æ’åºï¼ˆRerankingï¼‰
    5. Prompt æ„å»ºï¼ˆPrompt Buildingï¼‰
    6. LLM ç”Ÿæˆï¼ˆAnswer Generationï¼‰
    7. ç»“æœç¼“å­˜ï¼ˆResult Cachingï¼‰

    ğŸ’¡ ç‰¹æ€§ï¼š
    - æ”¯æŒåŒæ­¥/å¼‚æ­¥è°ƒç”¨
    - å¯é…ç½®çš„æ£€ç´¢ç­–ç•¥
    - è‡ªåŠ¨ç¼“å­˜ç®¡ç†
    - è¯¦ç»†çš„æ—¥å¿—è®°å½•
    """

    def __init__(
        self,
        embedding_model: Optional[EmbeddingModel] = None,
        llm_client: Optional[LLMClient] = None,
        bm25_retriever: Optional[BM25Retriever] = None,
        vector_retriever: Optional[VectorRetriever] = None,
        reranker: Optional[Reranker] = None,
        use_cache: bool = True,
        language: str = 'zh',
        enable_graph: bool = True,
        graph_weight: float = 0.3
    ):
        """
        åˆå§‹åŒ– RAG Pipeline

        å‚æ•°ï¼š
            embedding_model: Embedding æ¨¡å‹å®ä¾‹
            llm_client: LLM å®¢æˆ·ç«¯å®ä¾‹
            bm25_retriever: BM25 æ£€ç´¢å™¨å®ä¾‹
            vector_retriever: å‘é‡æ£€ç´¢å™¨å®ä¾‹
            reranker: é‡æ’åºå™¨å®ä¾‹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            language: å›ç­”è¯­è¨€ ('zh' æˆ– 'en')
            enable_graph: æ˜¯å¦å¯ç”¨å›¾è°±å¢å¼ºæ£€ç´¢
            graph_weight: å›¾è°±æ£€ç´¢ç»“æœæƒé‡ (0.0-1.0)
        """
        self.use_cache = use_cache
        self.language = language
        self.enable_graph = enable_graph and GRAPH_RETRIEVAL_AVAILABLE
        self.graph_weight = graph_weight

        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆæ‡’åŠ è½½ï¼‰
        self._embedding_model = embedding_model
        self._llm_client = llm_client
        self._bm25_retriever = bm25_retriever
        self._vector_retriever = vector_retriever
        self._reranker = reranker
        self._hybrid_retriever = None

        # å›¾è°±å¢å¼ºç»„ä»¶
        self._graph_retriever = None
        self._graph_enhanced_retriever = None

        # ç»„ä»¶åˆå§‹åŒ–æ ‡å¿—
        self._initialized = False

        logger.info(
            f"RAG Pipeline åˆ›å»º | "
            f"ç¼“å­˜: {use_cache} | "
            f"è¯­è¨€: {language} | "
            f"å›¾è°±å¢å¼º: {self.enable_graph}"
        )

    def _lazy_init(self):
        """
        æ‡’åŠ è½½åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶

        åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åˆå§‹åŒ–ï¼Œé¿å…å¯åŠ¨æ—¶çš„å¼€é”€
        """
        if self._initialized:
            return

        logger.info("åˆå§‹åŒ– RAG Pipeline ç»„ä»¶...")

        # 1. åˆå§‹åŒ– Embedding æ¨¡å‹
        if self._embedding_model is None:
            self._embedding_model = EmbeddingModel(
                model_name=settings.EMBEDDING_MODEL_NAME
            )

        # 2. åˆå§‹åŒ– Embedder
        self._embedder = Embedder(
            embedding_model=self._embedding_model,
            batch_size=settings.EMBEDDING_BATCH_SIZE
        )

        # 3. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        if self._llm_client is None:
            self._llm_client = LLMClient()

        # 4. åˆå§‹åŒ– BM25 æ£€ç´¢å™¨
        if self._bm25_retriever is None:
            self._bm25_retriever = BM25Retriever()

        # 5. åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨
        if self._vector_retriever is None:
            self._vector_retriever = VectorRetriever(
                collection_name=settings.MILVUS_COLLECTION_STANDARD,
                embedder=self._embedder,
                host=settings.MILVUS_HOST,
                port=str(settings.MILVUS_PORT),
                dim=settings.VECTOR_DIM
            )

        # 6. åˆå§‹åŒ–é‡æ’åºå™¨
        if self._reranker is None:
            try:
                self._reranker = Reranker()
            except Exception as e:
                logger.warning(f"Reranker åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨é‡æ’åº: {e}")
                self._reranker = None

        # 7. åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
        self._hybrid_retriever = HybridRetriever(
            bm25_retriever=self._bm25_retriever,
            vector_retriever=self._vector_retriever,
            reranker=self._reranker,
            fusion_method='rrf'
        )

        # 8. åˆå§‹åŒ–å›¾è°±å¢å¼ºç»„ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_graph and GRAPH_RETRIEVAL_AVAILABLE:
            try:
                # å›¾è°±æ£€ç´¢å™¨
                self._graph_retriever = GraphRetriever()

                # å›¾è°±å¢å¼ºæ£€ç´¢å™¨ï¼ˆä¸‰è·¯èåˆï¼‰
                self._graph_enhanced_retriever = GraphEnhancedRetriever(
                    bm25_retriever=self._bm25_retriever,
                    vector_retriever=self._vector_retriever,
                    graph_retriever=self._graph_retriever,
                    reranker=self._reranker,
                    graph_weight=self.graph_weight
                )
                logger.info(f"å›¾è°±å¢å¼ºæ£€ç´¢å™¨å·²å¯ç”¨ | å›¾è°±æƒé‡: {self.graph_weight}")
            except Exception as e:
                logger.warning(f"å›¾è°±å¢å¼ºç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
                self._graph_retriever = None
                self._graph_enhanced_retriever = None
                self.enable_graph = False

        self._initialized = True
        logger.info("RAG Pipeline ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    async def run(
        self,
        query: str,
        *,
        top_k: int = 5,
        project_id: Optional[str] = None,
        extra_context: Optional[str] = None,
        use_rerank: bool = True,
        skip_cache: bool = False,
        use_graph: Optional[bool] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œ RAG æµç¨‹ï¼ˆå¼‚æ­¥ï¼‰

        å‚æ•°ï¼š
            query: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            project_id: é¡¹ç›® IDï¼ˆç”¨äºé™å®šæ£€ç´¢èŒƒå›´ï¼‰
            extra_context: é¢å¤–ä¸Šä¸‹æ–‡
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            skip_cache: æ˜¯å¦è·³è¿‡ç¼“å­˜
            use_graph: æ˜¯å¦ä½¿ç”¨å›¾è°±å¢å¼ºï¼ˆNone è¡¨ç¤ºä½¿ç”¨é»˜è®¤é…ç½®ï¼‰

        è¿”å›ï¼š
            {
                'answer': str,           # ç”Ÿæˆçš„ç­”æ¡ˆ
                'sources': List[Dict],   # æ¥æºæ–‡æ¡£
                'query': str,            # åŸå§‹é—®é¢˜
                'cached': bool,          # æ˜¯å¦æ¥è‡ªç¼“å­˜
                'graph_context': str,    # å›¾è°±ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                'metadata': {
                    'retrieval_count': int,
                    'response_time': float,
                    'model': str,
                    'timestamp': str,
                    'graph_enhanced': bool
                }
            }
        """
        start_time = datetime.now()

        logger.info(f"RAG Pipeline å¼€å§‹ | é—®é¢˜: {query[:50]}...")

        # æ‡’åŠ è½½åˆå§‹åŒ–
        self._lazy_init()

        # Step 1: æ£€æŸ¥ç¼“å­˜
        if self.use_cache and not skip_cache:
            cached_result = self._check_cache(query)
            if cached_result:
                cached_result['cached'] = True
                logger.info("å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›")
                return cached_result

        # Step 2: æŸ¥è¯¢é¢„å¤„ç†
        processed_query = self._preprocess_query(query)

        # ç¡®å®šæ˜¯å¦ä½¿ç”¨å›¾è°±å¢å¼º
        should_use_graph = use_graph if use_graph is not None else self.enable_graph

        # Step 3: æ··åˆæ£€ç´¢ï¼ˆæ”¯æŒå›¾è°±å¢å¼ºï¼‰
        retrieved_docs, graph_context = await self._retrieve(
            query=processed_query,
            top_k=top_k,
            project_id=project_id,
            use_rerank=use_rerank,
            use_graph=should_use_graph
        )

        # Step 4: æ£€æŸ¥æ˜¯å¦æœ‰æ£€ç´¢ç»“æœ
        if not retrieved_docs:
            logger.warning("æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
            return self._generate_no_result_response(query, start_time)

        # Step 5: æ„å»º Promptï¼ˆåŒ…å«å›¾è°±ä¸Šä¸‹æ–‡ï¼‰
        prompt = self._build_prompt(
            query=query,
            contexts=retrieved_docs,
            extra_context=extra_context,
            graph_context=graph_context
        )

        # Step 6: LLM ç”Ÿæˆç­”æ¡ˆ
        answer = await self._generate_answer(prompt)

        # Step 7: æ„å»ºç»“æœ
        result = self._build_result(
            query=query,
            answer=answer,
            sources=retrieved_docs,
            start_time=start_time,
            graph_context=graph_context,
            graph_enhanced=should_use_graph and graph_context is not None
        )

        # Step 8: ç¼“å­˜ç»“æœ
        if self.use_cache:
            self._cache_result(query, result)

        logger.info(
            f"RAG Pipeline å®Œæˆ | "
            f"æ£€ç´¢: {len(retrieved_docs)} æ¡ | "
            f"è€—æ—¶: {result['metadata']['response_time']:.2f}s"
        )

        return result

    def run_sync(
        self,
        query: str,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œ RAG æµç¨‹ï¼ˆåŒæ­¥ï¼‰

        å‚æ•°ä¸ run() ç›¸åŒ
        """
        return asyncio.run(self.run(query, **kwargs))

    def _preprocess_query(self, query: str) -> str:
        """
        æŸ¥è¯¢é¢„å¤„ç†

        å¤„ç†ï¼š
        1. å»é™¤å¤šä½™ç©ºç™½
        2. æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        3. å¯é€‰ï¼šæŸ¥è¯¢æ‰©å±•
        """
        # åŸºç¡€æ¸…ç†
        processed = query.strip()
        processed = ' '.join(processed.split())

        return processed

    def _check_cache(self, query: str) -> Optional[Dict[str, Any]]:
        """æ£€æŸ¥ç¼“å­˜"""
        try:
            return redis_client.get_cached_query_result(query)
        except Exception as e:
            logger.warning(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e}")
            return None

    def _cache_result(self, query: str, result: Dict[str, Any]):
        """ç¼“å­˜ç»“æœ"""
        try:
            # ä¸ç¼“å­˜æŸäº›å­—æ®µ
            cache_data = {
                'answer': result['answer'],
                'sources': result['sources'],
                'query': result['query']
            }
            redis_client.cache_query_result(query, cache_data)
        except Exception as e:
            logger.warning(f"ç»“æœç¼“å­˜å¤±è´¥: {e}")

    async def _retrieve(
        self,
        query: str,
        top_k: int,
        project_id: Optional[str],
        use_rerank: bool,
        use_graph: bool = False
    ) -> tuple[List[Dict], Optional[str]]:
        """
        æ‰§è¡Œæ··åˆæ£€ç´¢ï¼ˆæ”¯æŒå›¾è°±å¢å¼ºï¼‰

        å‚æ•°ï¼š
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            project_id: é¡¹ç›®è¿‡æ»¤
            use_rerank: æ˜¯å¦é‡æ’åº
            use_graph: æ˜¯å¦ä½¿ç”¨å›¾è°±å¢å¼º

        è¿”å›ï¼š
            (æ£€ç´¢ç»“æœåˆ—è¡¨, å›¾è°±ä¸Šä¸‹æ–‡)
        """
        try:
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            filters = None
            if project_id:
                filters = f"project_id == '{project_id}'"

            graph_context = None

            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å›¾è°±å¢å¼ºæ£€ç´¢
            if use_graph and self._graph_enhanced_retriever is not None:
                logger.info("ä½¿ç”¨å›¾è°±å¢å¼ºä¸‰è·¯æ£€ç´¢")

                # ä½¿ç”¨å›¾è°±å¢å¼ºæ£€ç´¢å™¨ï¼ˆä¸‰è·¯èåˆï¼‰
                results = await self._graph_enhanced_retriever.search_async(
                    query=query,
                    top_k=top_k,
                    use_rerank=use_rerank,
                    filters=filters,
                    enhance_with_graph=True
                )

                # ä»æ£€ç´¢ç»“æœä¸­æå–å›¾è°±ä¸Šä¸‹æ–‡
                graph_context = self._graph_enhanced_retriever.get_graph_context_for_prompt(results)

                if graph_context:
                    logger.info(f"å›¾è°±ä¸Šä¸‹æ–‡ç”Ÿæˆå®Œæˆ | é•¿åº¦: {len(graph_context)}")
            else:
                # ä½¿ç”¨æ ‡å‡†æ··åˆæ£€ç´¢
                results = self._hybrid_retriever.search(
                    query=query,
                    top_k=top_k,
                    use_rerank=use_rerank,
                    filters=filters
                )

            return results, graph_context

        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            return [], None

    def _build_prompt(
        self,
        query: str,
        contexts: List[Dict],
        extra_context: Optional[str],
        graph_context: Optional[str] = None
    ) -> str:
        """
        æ„å»º LLM Prompt

        ä½¿ç”¨ QAPromptFactory æ„å»ºæ ‡å‡†åŒ–çš„ Prompt
        æ”¯æŒå›¾è°±ä¸Šä¸‹æ–‡å¢å¼º
        """
        # æå–ä¸Šä¸‹æ–‡æ–‡æœ¬
        context_items = []
        for i, doc in enumerate(contexts, 1):
            text = doc.get('text', '')
            source = doc.get('doc_id', f'æ–‡æ¡£{i}')
            score = doc.get('rerank_score', doc.get('score', 0))

            context_items.append({
                'text': text,
                'metadata': {
                    'source': source,
                    'score': score
                }
            })

        # æ„å»º Prompt
        prompt = QAPromptFactory.build_rag_prompt(
            query=query,
            contexts=context_items,
            language=self.language,
            max_context_length=3000,
            include_metadata=True
        )

        # æ·»åŠ å›¾è°±çŸ¥è¯†ä¸Šä¸‹æ–‡ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        if graph_context:
            graph_section = self._format_graph_context(graph_context)
            prompt = f"{graph_section}\n\n{prompt}"

        # æ·»åŠ é¢å¤–ä¸Šä¸‹æ–‡
        if extra_context:
            prompt = f"{prompt}\n\nã€é¢å¤–ä¿¡æ¯ã€‘\n{extra_context}"

        return prompt

    def _format_graph_context(self, graph_context: str) -> str:
        """
        æ ¼å¼åŒ–å›¾è°±ä¸Šä¸‹æ–‡

        å°†å›¾è°±çŸ¥è¯†ä»¥ç»“æ„åŒ–æ–¹å¼åµŒå…¥ Prompt
        """
        if self.language == 'zh':
            return f"""ã€çŸ¥è¯†å›¾è°±å‚è€ƒã€‘
ä»¥ä¸‹æ˜¯ä»å·¥ç¨‹çŸ¥è¯†å›¾è°±ä¸­æå–çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆå‚è€ƒï¼š

{graph_context}

---"""
        else:
            return f"""ã€Knowledge Graph Referenceã€‘
The following structured information is extracted from the engineering knowledge graph. Please prioritize this:

{graph_context}

---"""

    async def _generate_answer(self, prompt: str) -> str:
        """
        è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
        """
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "system",
                    "content": self._get_system_prompt()
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]

            # è°ƒç”¨ LLM
            answer = await self._llm_client.chat_async(messages=messages)

            return answer

        except Exception as e:
            logger.error(f"LLM ç”Ÿæˆå¤±è´¥: {e}")
            return self._get_fallback_answer()

    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿ Prompt"""
        if self.language == 'zh':
            return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. å¿…é¡»åŸºäºå‚è€ƒèµ„æ–™å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¼•ç”¨å…·ä½“å†…å®¹æ—¶ï¼Œæ ‡æ³¨æ¥æº
4. å›ç­”è¦å‡†ç¡®ã€ä¸“ä¸šã€æ˜“æ‡‚"""
        else:
            return """You are a professional knowledge assistant. Answer questions accurately based on provided references.

Requirements:
1. Must answer based on references, do not fabricate
2. Clearly state if references are insufficient
3. Cite sources when quoting content
4. Be accurate, professional, and clear"""

    def _get_fallback_answer(self) -> str:
        """è·å–é™çº§ç­”æ¡ˆ"""
        if self.language == 'zh':
            return "æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•ç”Ÿæˆç­”æ¡ˆï¼Œè¯·ç¨åé‡è¯•ã€‚"
        else:
            return "Sorry, the system is temporarily unable to generate an answer. Please try again later."

    def _build_result(
        self,
        query: str,
        answer: str,
        sources: List[Dict],
        start_time: datetime,
        graph_context: Optional[str] = None,
        graph_enhanced: bool = False
    ) -> Dict[str, Any]:
        """æ„å»ºè¿”å›ç»“æœ"""
        end_time = datetime.now()
        response_time = (end_time - start_time).total_seconds()

        result = {
            'answer': answer,
            'sources': [
                {
                    'doc_id': doc.get('doc_id', ''),
                    'text': doc.get('text', '')[:500],  # æˆªæ–­
                    'score': doc.get('rerank_score', doc.get('score', 0)),
                    'metadata': doc.get('metadata', {}),
                    'from_graph': doc.get('from_graph', False)  # æ ‡è®°æ˜¯å¦æ¥è‡ªå›¾è°±
                }
                for doc in sources
            ],
            'query': query,
            'cached': False,
            'metadata': {
                'retrieval_count': len(sources),
                'response_time': response_time,
                'model': self._llm_client.model if self._llm_client else 'unknown',
                'timestamp': end_time.isoformat(),
                'graph_enhanced': graph_enhanced
            }
        }

        # æ·»åŠ å›¾è°±ä¸Šä¸‹æ–‡æ‘˜è¦
        if graph_context:
            result['graph_context'] = graph_context[:500] if len(graph_context) > 500 else graph_context

        return result

    def _generate_no_result_response(
        self,
        query: str,
        start_time: datetime
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ— ç»“æœçš„å“åº”"""
        end_time = datetime.now()
        response_time = (end_time - start_time).total_seconds()

        if self.language == 'zh':
            answer = "æŠ±æ­‰ï¼Œæœªèƒ½åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸€ç§é—®æ³•ï¼Œæˆ–ç¡®è®¤é—®é¢˜æ˜¯å¦åœ¨çŸ¥è¯†åº“è¦†ç›–èŒƒå›´å†…ã€‚"
        else:
            answer = "Sorry, no relevant content was found in the knowledge base. Please try rephrasing your question."

        return {
            'answer': answer,
            'sources': [],
            'query': query,
            'cached': False,
            'metadata': {
                'retrieval_count': 0,
                'response_time': response_time,
                'model': 'none',
                'timestamp': end_time.isoformat(),
                'no_result': True,
                'graph_enhanced': False
            }
        }


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
# 1. åŸºç¡€ä½¿ç”¨ï¼ˆå¼‚æ­¥ï¼‰
from services.rag.pipeline import RagPipeline
import asyncio

pipeline = RagPipeline()

async def main():
    result = await pipeline.run(
        query="å»ºç­‘ç»“æ„è·è½½å¦‚ä½•è®¡ç®—ï¼Ÿ",
        top_k=5
    )
    print(f"ç­”æ¡ˆ: {result['answer']}")
    print(f"æ¥æºæ•°: {len(result['sources'])}")

asyncio.run(main())


# 2. åŒæ­¥è°ƒç”¨
result = pipeline.run_sync(
    query="ä»€ä¹ˆæ˜¯æ’è·è½½ï¼Ÿ",
    top_k=5
)
print(result['answer'])


# 3. é™å®šé¡¹ç›®èŒƒå›´
result = await pipeline.run(
    query="é¡¹ç›®è¿›åº¦å¦‚ä½•ï¼Ÿ",
    top_k=5,
    project_id="project_001"
)


# 4. è·³è¿‡ç¼“å­˜
result = await pipeline.run(
    query="æœ€æ–°çš„è§„èŒƒè¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
    skip_cache=True
)


# 5. æ·»åŠ é¢å¤–ä¸Šä¸‹æ–‡
result = await pipeline.run(
    query="å¦‚ä½•è®¡ç®—æ¥¼é¢è·è½½ï¼Ÿ",
    extra_context="å½“å‰é¡¹ç›®ä¸ºä½å®…æ¥¼ï¼Œåœ°ä¸Š20å±‚"
)


# 6. è‡ªå®šä¹‰ç»„ä»¶
from services.llm.llm_client import LLMClient
from services.embedding.embedding_model import EmbeddingModel

custom_llm = LLMClient(
    api_base="http://localhost:8000/v1",
    model="qwen-plus"
)

custom_embedding = EmbeddingModel(
    model_name="BAAI/bge-large-zh-v1.5"
)

pipeline = RagPipeline(
    llm_client=custom_llm,
    embedding_model=custom_embedding,
    use_cache=False,
    language='zh'
)

result = await pipeline.run(query="...")


# 7. å›¾è°±å¢å¼ºæ£€ç´¢ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
pipeline = RagPipeline(
    enable_graph=True,      # å¯ç”¨å›¾è°±å¢å¼º
    graph_weight=0.3        # å›¾è°±ç»“æœæƒé‡
)

result = await pipeline.run(
    query="KL-1 æ¡†æ¶æ¢ä½¿ç”¨ä»€ä¹ˆææ–™ï¼Ÿ",
    top_k=5
)
print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"å›¾è°±å¢å¼º: {result['metadata']['graph_enhanced']}")
if 'graph_context' in result:
    print(f"å›¾è°±ä¸Šä¸‹æ–‡: {result['graph_context']}")


# 8. ç¦ç”¨å›¾è°±å¢å¼ºï¼ˆå•æ¬¡æŸ¥è¯¢ï¼‰
result = await pipeline.run(
    query="ä»€ä¹ˆæ˜¯å‰ªåŠ›å¢™ï¼Ÿ",
    use_graph=False  # æœ¬æ¬¡æŸ¥è¯¢ä¸ä½¿ç”¨å›¾è°±
)


# 9. å®Œå…¨ç¦ç”¨å›¾è°±å¢å¼º
pipeline = RagPipeline(
    enable_graph=False  # å®Œå…¨ç¦ç”¨å›¾è°±
)
"""
