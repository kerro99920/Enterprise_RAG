"""
========================================
Embeddingæ¨¡å‹ç®¡ç†å™¨
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- åŠ è½½å’Œç®¡ç†å‘é‡åŒ–æ¨¡å‹
- æ”¯æŒå¤šç§Embeddingæ¨¡å‹
- ç»Ÿä¸€çš„å‘é‡åŒ–æ¥å£

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ¨¡å‹åŠ è½½å’Œç¼“å­˜
2. æ‰¹é‡å‘é‡åŒ–
3. ç›¸ä¼¼åº¦è®¡ç®—
4. æ¨¡å‹åˆ‡æ¢

========================================
"""

import os
from typing import List, Union, Optional, Dict
from pathlib import Path

import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from loguru import logger


class EmbeddingModel:
    """
    Embeddingæ¨¡å‹ç®¡ç†å™¨

    ğŸ”§ æ”¯æŒçš„æ¨¡å‹ï¼š
    - BAAI/bge-large-zh-v1.5 (æ¨èä¸­æ–‡)
    - BAAI/bge-base-zh-v1.5
    - shibing624/text2vec-base-chinese
    - å…¶ä»–SentenceTransformerå…¼å®¹æ¨¡å‹

    ğŸ’¡ ç‰¹æ€§ï¼š
    - è‡ªåŠ¨è®¾å¤‡é€‰æ‹©ï¼ˆGPU/CPUï¼‰
    - æ‰¹é‡å¤„ç†ä¼˜åŒ–
    - æ¨¡å‹ç¼“å­˜
    """

    # æ¨èæ¨¡å‹é…ç½®
    RECOMMENDED_MODELS = {
        'bge-large-zh': {
            'model_name': 'BAAI/bge-large-zh-v1.5',
            'dimension': 1024,
            'max_length': 512,
            'description': 'BGEå¤§æ¨¡å‹ï¼Œä¸­æ–‡æœ€ä½³æ€§èƒ½'
        },
        'bge-base-zh': {
            'model_name': 'BAAI/bge-base-zh-v1.5',
            'dimension': 768,
            'max_length': 512,
            'description': 'BGEåŸºç¡€æ¨¡å‹ï¼Œé€Ÿåº¦å¿«'
        },
        'text2vec': {
            'model_name': 'shibing624/text2vec-base-chinese',
            'dimension': 768,
            'max_length': 256,
            'description': 'Text2Vecï¼Œè½»é‡çº§'
        }
    }

    def __init__(
            self,
            model_name: str = 'BAAI/bge-large-zh-v1.5',
            device: Optional[str] = None,
            cache_dir: Optional[str] = None,
            normalize_embeddings: bool = True
    ):
        """
        åˆå§‹åŒ–Embeddingæ¨¡å‹

        å‚æ•°ï¼š
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è®¾å¤‡ ('cuda', 'cpu', 'mps' æˆ– Noneè‡ªåŠ¨é€‰æ‹©)
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            normalize_embeddings: æ˜¯å¦å½’ä¸€åŒ–å‘é‡ï¼ˆæ¨èTrueï¼‰
        """
        self.model_name = model_name
        self.normalize_embeddings = normalize_embeddings

        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if device is None:
            if torch.cuda.is_available():
                device = 'cuda'
            elif torch.backends.mps.is_available():
                device = 'mps'
            else:
                device = 'cpu'

        self.device = device
        self.cache_dir = cache_dir or os.path.join(
            Path.home(),
            '.cache',
            'huggingface',
            'hub'
        )

        logger.info(
            f"åˆå§‹åŒ–Embeddingæ¨¡å‹ | "
            f"æ¨¡å‹: {model_name} | "
            f"è®¾å¤‡: {device} | "
            f"å½’ä¸€åŒ–: {normalize_embeddings}"
        )

        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()
        self.dimension = self.model.get_sentence_embedding_dimension()

        logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆ | å‘é‡ç»´åº¦: {self.dimension}")

    def _load_model(self) -> SentenceTransformer:
        """åŠ è½½SentenceTransformeræ¨¡å‹"""
        try:
            model = SentenceTransformer(
                self.model_name,
                device=self.device,
                cache_folder=self.cache_dir
            )

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            model.eval()

            return model

        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def encode(
            self,
            texts: Union[str, List[str]],
            batch_size: int = 32,
            show_progress: bool = False,
            convert_to_numpy: bool = True
    ) -> Union[np.ndarray, torch.Tensor]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡

        å‚æ•°ï¼š
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            convert_to_numpy: æ˜¯å¦è½¬ä¸ºnumpyæ•°ç»„

        è¿”å›ï¼š
            å‘é‡æ•°ç»„ shape=(n, dimension)
        """
        # ç»Ÿä¸€å¤„ç†ä¸ºåˆ—è¡¨
        if isinstance(texts, str):
            texts = [texts]
            single_text = True
        else:
            single_text = False

        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [t for t in texts if t and t.strip()]
        if not valid_texts:
            logger.warning("è¾“å…¥åŒ…å«ç©ºæ–‡æœ¬ï¼Œè¿”å›é›¶å‘é‡")
            return np.zeros((len(texts), self.dimension))

        logger.debug(f"ç¼–ç æ–‡æœ¬ | æ•°é‡: {len(valid_texts)} | batch_size: {batch_size}")

        try:
            # ä½¿ç”¨æ¨¡å‹ç¼–ç 
            embeddings = self.model.encode(
                valid_texts,
                batch_size=batch_size,
                show_progress_bar=show_progress,
                convert_to_numpy=convert_to_numpy,
                normalize_embeddings=self.normalize_embeddings
            )

            # å¦‚æœæ˜¯å•ä¸ªæ–‡æœ¬ï¼Œè¿”å›ä¸€ç»´å‘é‡
            if single_text and convert_to_numpy:
                return embeddings[0]

            return embeddings

        except Exception as e:
            logger.error(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            raise

    def encode_queries(
            self,
            queries: Union[str, List[str]],
            **kwargs
    ) -> Union[np.ndarray, torch.Tensor]:
        """
        ç¼–ç æŸ¥è¯¢æ–‡æœ¬ï¼ˆä¸ºæŸ¥è¯¢ä¼˜åŒ–ï¼‰

        æ³¨æ„ï¼šæŸäº›æ¨¡å‹å¯¹æŸ¥è¯¢å’Œæ–‡æ¡£ä½¿ç”¨ä¸åŒçš„ç¼–ç 
        """
        # BGEæ¨¡å‹éœ€è¦æ·»åŠ æŸ¥è¯¢æŒ‡ä»¤
        if 'bge' in self.model_name.lower():
            if isinstance(queries, str):
                queries = f"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š{queries}"
            else:
                queries = [
                    f"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š{q}"
                    for q in queries
                ]

        return self.encode(queries, **kwargs)

    def similarity(
            self,
            embeddings1: np.ndarray,
            embeddings2: np.ndarray,
            metric: str = 'cosine'
    ) -> Union[float, np.ndarray]:
        """
        è®¡ç®—å‘é‡ç›¸ä¼¼åº¦

        å‚æ•°ï¼š
            embeddings1: å‘é‡1æˆ–å‘é‡çŸ©é˜µ1
            embeddings2: å‘é‡2æˆ–å‘é‡çŸ©é˜µ2
            metric: ç›¸ä¼¼åº¦åº¦é‡ ('cosine', 'dot', 'euclidean')

        è¿”å›ï¼š
            ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if metric == 'cosine':
            # ä½™å¼¦ç›¸ä¼¼åº¦
            if self.normalize_embeddings:
                # å¦‚æœå·²å½’ä¸€åŒ–ï¼Œç›´æ¥ç‚¹ç§¯
                return np.dot(embeddings1, embeddings2.T)
            else:
                # æœªå½’ä¸€åŒ–ï¼Œè®¡ç®—ä½™å¼¦
                norm1 = np.linalg.norm(embeddings1, axis=-1, keepdims=True)
                norm2 = np.linalg.norm(embeddings2, axis=-1, keepdims=True)
                return np.dot(embeddings1, embeddings2.T) / (norm1 * norm2.T)

        elif metric == 'dot':
            # ç‚¹ç§¯
            return np.dot(embeddings1, embeddings2.T)

        elif metric == 'euclidean':
            # æ¬§æ°è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰
            return -np.linalg.norm(
                embeddings1[:, None] - embeddings2,
                axis=-1
            )

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦åº¦é‡: {metric}")

    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_name': self.model_name,
            'dimension': self.dimension,
            'device': self.device,
            'normalize_embeddings': self.normalize_embeddings,
            'max_seq_length': self.model.max_seq_length
        }

    @classmethod
    def list_recommended_models(cls) -> Dict:
        """åˆ—å‡ºæ¨èçš„æ¨¡å‹é…ç½®"""
        return cls.RECOMMENDED_MODELS

    def __repr__(self) -> str:
        return (
            f"EmbeddingModel("
            f"model='{self.model_name}', "
            f"dim={self.dimension}, "
            f"device='{self.device}')"
        )


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from services.embedding.embedding_model import EmbeddingModel

# 1. åŸºç¡€ä½¿ç”¨
model = EmbeddingModel(
    model_name='BAAI/bge-large-zh-v1.5',
    device='cuda'  # æˆ– 'cpu'
)

# ç¼–ç å•ä¸ªæ–‡æœ¬
text = "å»ºç­‘ç»“æ„è·è½½è§„èŒƒ"
embedding = model.encode(text)
print(f"å‘é‡ç»´åº¦: {embedding.shape}")  # (1024,)

# ç¼–ç å¤šä¸ªæ–‡æœ¬
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
embeddings = model.encode(texts, batch_size=32)
print(f"å‘é‡çŸ©é˜µ: {embeddings.shape}")  # (3, 1024)


# 2. æŸ¥è¯¢ç¼–ç ï¼ˆä¸ºæ£€ç´¢ä¼˜åŒ–ï¼‰
query = "ä»€ä¹ˆæ˜¯å»ºç­‘è·è½½ï¼Ÿ"
query_embedding = model.encode_queries(query)


# 3. ç›¸ä¼¼åº¦è®¡ç®—
text1 = "å»ºç­‘ç»“æ„è®¾è®¡"
text2 = "ç»“æ„è·è½½è®¡ç®—"

emb1 = model.encode(text1)
emb2 = model.encode(text2)

similarity = model.similarity(emb1, emb2)
print(f"ç›¸ä¼¼åº¦: {similarity:.4f}")


# 4. æ‰¹é‡ç›¸ä¼¼åº¦
query_emb = model.encode("å»ºç­‘")
doc_embs = model.encode(["å»ºç­‘è®¾è®¡", "è½¯ä»¶å¼€å‘", "ç»“æ„å·¥ç¨‹"])

similarities = model.similarity(query_emb, doc_embs)
print(f"ç›¸ä¼¼åº¦: {similarities}")


# 5. æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
info = model.get_model_info()
print(f"æ¨¡å‹ä¿¡æ¯: {info}")


# 6. æŸ¥çœ‹æ¨èæ¨¡å‹
models = EmbeddingModel.list_recommended_models()
for key, config in models.items():
    print(f"{key}: {config['description']}")
"""