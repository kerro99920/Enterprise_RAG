"""
========================================
å‘é‡åŒ–æœåŠ¡
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- æ–‡æ¡£åˆ†å—å‘é‡åŒ–
- æ‰¹é‡å¤„ç†ä¼˜åŒ–
- å‘é‡ç¼“å­˜ç®¡ç†

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ–‡æ¡£çº§å‘é‡åŒ–
2. åˆ†å—å‘é‡åŒ–
3. å¢é‡å‘é‡åŒ–
4. å‘é‡æŒä¹…åŒ–

========================================
"""

import json
from typing import List, Dict, Optional, Tuple
from pathlib import Path

import numpy as np
from loguru import logger

from services.embedding.embedding_model import EmbeddingModel
from services.document.splitter import Chunk


class Embedder:
    """
    å‘é‡åŒ–æœåŠ¡

    ğŸ”§ æ ¸å¿ƒèŒè´£ï¼š
    - å°†æ–‡æ¡£å—è½¬æ¢ä¸ºå‘é‡
    - ç®¡ç†å‘é‡åŒ–æµç¨‹
    - ä¼˜åŒ–æ‰¹å¤„ç†æ€§èƒ½

    ğŸ’¡ ç‰¹æ€§ï¼š
    - è‡ªåŠ¨æ‰¹å¤„ç†
    - å‘é‡ç¼“å­˜
    - é”™è¯¯é‡è¯•
    """

    def __init__(
            self,
            embedding_model: Optional[EmbeddingModel] = None,
            batch_size: int = 32,
            cache_dir: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–å‘é‡åŒ–æœåŠ¡

        å‚æ•°ï¼š
            embedding_model: Embeddingæ¨¡å‹å®ä¾‹
            batch_size: æ‰¹å¤„ç†å¤§å°
            cache_dir: å‘é‡ç¼“å­˜ç›®å½•
        """
        self.embedding_model = embedding_model or EmbeddingModel()
        self.batch_size = batch_size
        self.cache_dir = Path(cache_dir) if cache_dir else None

        if self.cache_dir:
            self.cache_dir.mkdir(parents=True, exist_ok=True)

        logger.info(
            f"åˆå§‹åŒ–å‘é‡åŒ–æœåŠ¡ | "
            f"æ¨¡å‹: {self.embedding_model.model_name} | "
            f"batch_size: {batch_size}"
        )

    def embed_chunks(
            self,
            chunks: List[Chunk],
            show_progress: bool = True
    ) -> List[Dict]:
        """
        å‘é‡åŒ–æ–‡æ¡£å—åˆ—è¡¨

        å‚æ•°ï¼š
            chunks: æ–‡æ¡£å—åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦

        è¿”å›ï¼š
            åŒ…å«å‘é‡çš„å—æ•°æ®åˆ—è¡¨
            [
                {
                    'text': str,
                    'embedding': np.ndarray,
                    'metadata': dict,
                    ...
                }
            ]
        """
        if not chunks:
            logger.warning("è¾“å…¥å—åˆ—è¡¨ä¸ºç©º")
            return []

        logger.info(f"å¼€å§‹å‘é‡åŒ– | å—æ•°: {len(chunks)}")

        # æå–æ–‡æœ¬
        texts = [chunk.text for chunk in chunks]

        # æ‰¹é‡ç¼–ç 
        try:
            embeddings = self.embedding_model.encode(
                texts,
                batch_size=self.batch_size,
                show_progress=show_progress
            )

            # ç»„åˆç»“æœ
            embedded_chunks = []
            for chunk, embedding in zip(chunks, embeddings):
                embedded_chunk = {
                    'text': chunk.text,
                    'embedding': embedding,
                    'start_idx': chunk.start_idx,
                    'end_idx': chunk.end_idx,
                    'metadata': chunk.metadata.copy(),
                    'embedding_dim': len(embedding)
                }
                embedded_chunks.append(embedded_chunk)

            logger.info(
                f"å‘é‡åŒ–å®Œæˆ | "
                f"å—æ•°: {len(embedded_chunks)} | "
                f"å‘é‡ç»´åº¦: {len(embeddings[0])}"
            )

            return embedded_chunks

        except Exception as e:
            logger.error(f"å‘é‡åŒ–å¤±è´¥: {e}")
            raise

    def embed_documents(
            self,
            documents: List[Dict],
            text_key: str = 'text',
            metadata_key: str = 'metadata'
    ) -> List[Dict]:
        """
        å‘é‡åŒ–æ–‡æ¡£åˆ—è¡¨ï¼ˆæ•´æ–‡æ¡£ï¼Œéåˆ†å—ï¼‰

        å‚æ•°ï¼š
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¯å­—å…¸
            text_key: æ–‡æœ¬å­—æ®µçš„é”®å
            metadata_key: å…ƒæ•°æ®å­—æ®µçš„é”®å

        è¿”å›ï¼š
            åŒ…å«å‘é‡çš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return []

        logger.info(f"å‘é‡åŒ–æ–‡æ¡£ | æ•°é‡: {len(documents)}")

        # æå–æ–‡æœ¬
        texts = [doc.get(text_key, '') for doc in documents]

        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_indices = [i for i, t in enumerate(texts) if t and t.strip()]
        valid_texts = [texts[i] for i in valid_indices]

        if not valid_texts:
            logger.warning("æ‰€æœ‰æ–‡æ¡£æ–‡æœ¬ä¸ºç©º")
            return documents

        # æ‰¹é‡ç¼–ç 
        embeddings = self.embedding_model.encode(
            valid_texts,
            batch_size=self.batch_size,
            show_progress=True
        )

        # æ·»åŠ å‘é‡åˆ°æ–‡æ¡£
        embedding_idx = 0
        for doc_idx, doc in enumerate(documents):
            if doc_idx in valid_indices:
                doc['embedding'] = embeddings[embedding_idx]
                doc['embedding_dim'] = len(embeddings[embedding_idx])
                embedding_idx += 1
            else:
                # ç©ºæ–‡æ¡£ï¼Œä½¿ç”¨é›¶å‘é‡
                doc['embedding'] = np.zeros(self.embedding_model.dimension)
                doc['embedding_dim'] = self.embedding_model.dimension

        logger.info(f"æ–‡æ¡£å‘é‡åŒ–å®Œæˆ | æˆåŠŸ: {len(valid_indices)}/{len(documents)}")

        return documents

    def embed_query(self, query: str) -> np.ndarray:
        """
        å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬ï¼ˆå•ä¸ªï¼‰

        å‚æ•°ï¼š
            query: æŸ¥è¯¢æ–‡æœ¬

        è¿”å›ï¼š
            æŸ¥è¯¢å‘é‡
        """
        if not query or not query.strip():
            logger.warning("æŸ¥è¯¢æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡")
            return np.zeros(self.embedding_model.dimension)

        logger.debug(f"å‘é‡åŒ–æŸ¥è¯¢: {query[:50]}...")

        return self.embedding_model.encode_queries(query)

    def embed_queries(self, queries: List[str]) -> np.ndarray:
        """
        æ‰¹é‡å‘é‡åŒ–æŸ¥è¯¢

        å‚æ•°ï¼š
            queries: æŸ¥è¯¢åˆ—è¡¨

        è¿”å›ï¼š
            æŸ¥è¯¢å‘é‡çŸ©é˜µ shape=(n, dimension)
        """
        if not queries:
            return np.array([])

        logger.debug(f"æ‰¹é‡å‘é‡åŒ–æŸ¥è¯¢ | æ•°é‡: {len(queries)}")

        return self.embedding_model.encode_queries(
            queries,
            batch_size=self.batch_size
        )

    def save_embeddings(
            self,
            embedded_chunks: List[Dict],
            filename: str
    ) -> str:
        """
        ä¿å­˜å‘é‡åˆ°æ–‡ä»¶

        å‚æ•°ï¼š
            embedded_chunks: åŒ…å«å‘é‡çš„å—åˆ—è¡¨
            filename: æ–‡ä»¶å

        è¿”å›ï¼š
            ä¿å­˜è·¯å¾„
        """
        if not self.cache_dir:
            raise ValueError("æœªè®¾ç½®cache_dirï¼Œæ— æ³•ä¿å­˜å‘é‡")

        filepath = self.cache_dir / filename

        # å°†numpyæ•°ç»„è½¬ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        serializable_chunks = []
        for chunk in embedded_chunks:
            chunk_copy = chunk.copy()
            if isinstance(chunk_copy.get('embedding'), np.ndarray):
                chunk_copy['embedding'] = chunk_copy['embedding'].tolist()
            serializable_chunks.append(chunk_copy)

        # ä¿å­˜ä¸ºJSON
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(serializable_chunks, f, ensure_ascii=False, indent=2)

        logger.info(f"å‘é‡å·²ä¿å­˜: {filepath}")

        return str(filepath)

    def load_embeddings(self, filename: str) -> List[Dict]:
        """
        ä»æ–‡ä»¶åŠ è½½å‘é‡

        å‚æ•°ï¼š
            filename: æ–‡ä»¶å

        è¿”å›ï¼š
            åŒ…å«å‘é‡çš„å—åˆ—è¡¨
        """
        if not self.cache_dir:
            raise ValueError("æœªè®¾ç½®cache_dirï¼Œæ— æ³•åŠ è½½å‘é‡")

        filepath = self.cache_dir / filename

        if not filepath.exists():
            raise FileNotFoundError(f"å‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")

        with open(filepath, 'r', encoding='utf-8') as f:
            chunks = json.load(f)

        # å°†åˆ—è¡¨è½¬å›numpyæ•°ç»„
        for chunk in chunks:
            if 'embedding' in chunk and isinstance(chunk['embedding'], list):
                chunk['embedding'] = np.array(chunk['embedding'])

        logger.info(f"å‘é‡å·²åŠ è½½: {filepath} | å—æ•°: {len(chunks)}")

        return chunks

    def get_embedding_stats(
            self,
            embedded_chunks: List[Dict]
    ) -> Dict:
        """
        è·å–å‘é‡ç»Ÿè®¡ä¿¡æ¯

        å‚æ•°ï¼š
            embedded_chunks: åŒ…å«å‘é‡çš„å—åˆ—è¡¨

        è¿”å›ï¼š
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not embedded_chunks:
            return {}

        embeddings = np.array([c['embedding'] for c in embedded_chunks])

        stats = {
            'count': len(embedded_chunks),
            'dimension': embeddings.shape[1],
            'mean_norm': float(np.mean(np.linalg.norm(embeddings, axis=1))),
            'std_norm': float(np.std(np.linalg.norm(embeddings, axis=1))),
            'min_norm': float(np.min(np.linalg.norm(embeddings, axis=1))),
            'max_norm': float(np.max(np.linalg.norm(embeddings, axis=1)))
        }

        return stats


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from services.embedding.embedder import Embedder
from services.embedding.embedding_model import EmbeddingModel
from services.document.splitter import DocumentSplitter

# 1. åˆå§‹åŒ–
model = EmbeddingModel(model_name='BAAI/bge-large-zh-v1.5')
embedder = Embedder(
    embedding_model=model,
    batch_size=32,
    cache_dir='data/embeddings'
)

# 2. å‘é‡åŒ–æ–‡æ¡£å—
splitter = DocumentSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split(long_text, method='recursive')

embedded_chunks = embedder.embed_chunks(chunks, show_progress=True)

print(f"å‘é‡åŒ–å®Œæˆ: {len(embedded_chunks)}ä¸ªå—")
print(f"å‘é‡ç»´åº¦: {embedded_chunks[0]['embedding_dim']}")


# 3. å‘é‡åŒ–æŸ¥è¯¢
query = "å»ºç­‘è·è½½å¦‚ä½•è®¡ç®—ï¼Ÿ"
query_embedding = embedder.embed_query(query)
print(f"æŸ¥è¯¢å‘é‡: {query_embedding.shape}")


# 4. ä¿å­˜å’ŒåŠ è½½å‘é‡
embedder.save_embeddings(embedded_chunks, 'doc_chunks.json')
loaded_chunks = embedder.load_embeddings('doc_chunks.json')


# 5. è·å–ç»Ÿè®¡ä¿¡æ¯
stats = embedder.get_embedding_stats(embedded_chunks)
print(f"å‘é‡ç»Ÿè®¡: {stats}")


# 6. æ‰¹é‡å‘é‡åŒ–æ–‡æ¡£
documents = [
    {'text': 'æ–‡æ¡£1å†…å®¹', 'metadata': {'id': 1}},
    {'text': 'æ–‡æ¡£2å†…å®¹', 'metadata': {'id': 2}}
]

embedded_docs = embedder.embed_documents(documents)
for doc in embedded_docs:
    print(f"æ–‡æ¡£ID: {doc['metadata']['id']}, å‘é‡ç»´åº¦: {doc['embedding_dim']}")
"""