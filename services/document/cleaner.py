"""
========================================
æ–‡æ¡£æ¸…æ´—å™¨
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- æ¸…æ´—æ–‡æ¡£ä¸­çš„å™ªå£°å’Œæ— ç”¨ä¿¡æ¯
- ç»Ÿä¸€æ–‡æœ¬æ ¼å¼
- æé«˜æ£€ç´¢å’Œç”Ÿæˆè´¨é‡

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå™ªå£°
2. ä¿®å¤å¸¸è§æ ¼å¼é—®é¢˜
3. æ ‡å‡†åŒ–ç©ºç™½ç¬¦
4. å»é™¤é‡å¤å†…å®¹

========================================
"""

import re
from typing import List, Dict, Optional
from loguru import logger


class DocumentCleaner:
    """
    æ–‡æ¡£æ¸…æ´—å™¨

    ğŸ”§ æ¸…æ´—ç­–ç•¥ï¼š
    - å»é™¤æ— æ•ˆå­—ç¬¦
    - ä¿®å¤æ¢è¡Œé—®é¢˜
    - ç»Ÿä¸€ç©ºç™½ç¬¦
    - å»é™¤é‡å¤æ®µè½

    ğŸ’¡ è®¾è®¡åŸåˆ™ï¼š
    - ä¿ç•™æœ‰æ„ä¹‰çš„å†…å®¹
    - ä¿®å¤è€Œéåˆ é™¤
    - å¯é…ç½®æ¸…æ´—å¼ºåº¦
    """

    def __init__(
            self,
            remove_urls: bool = True,
            remove_emails: bool = True,
            fix_encoding: bool = True,
            remove_duplicates: bool = True,
            min_line_length: int = 2
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£æ¸…æ´—å™¨

        å‚æ•°ï¼š
            remove_urls: æ˜¯å¦åˆ é™¤URL
            remove_emails: æ˜¯å¦åˆ é™¤é‚®ç®±
            fix_encoding: æ˜¯å¦ä¿®å¤ç¼–ç é—®é¢˜
            remove_duplicates: æ˜¯å¦å»é™¤é‡å¤æ®µè½
            min_line_length: æœ€å°è¡Œé•¿åº¦ï¼ˆçŸ­äºæ­¤é•¿åº¦çš„è¡Œä¼šè¢«åˆ é™¤ï¼‰
        """
        self.remove_urls = remove_urls
        self.remove_emails = remove_emails
        self.fix_encoding = fix_encoding
        self.remove_duplicates = remove_duplicates
        self.min_line_length = min_line_length

        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæé«˜æ€§èƒ½ï¼‰
        self._compile_patterns()

    def _compile_patterns(self):
        """é¢„ç¼–è¯‘å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼"""
        # URLåŒ¹é…
        self.url_pattern = re.compile(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        )

        # é‚®ç®±åŒ¹é…
        self.email_pattern = re.compile(
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        )

        # å¤šä½™ç©ºç™½ç¬¦
        self.whitespace_pattern = re.compile(r'\s+')

        # è¿ç»­æ ‡ç‚¹ç¬¦å·
        self.punct_pattern = re.compile(r'([ã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼š])\1+')

        # é¡µç æ¨¡å¼
        self.page_number_pattern = re.compile(r'^\s*[-â€“â€”]\s*\d+\s*[-â€“â€”]\s*$')

        # é¡µçœ‰é¡µè„šï¼ˆå¸¸è§æ¨¡å¼ï¼‰
        self.header_footer_pattern = re.compile(
            r'(ç¬¬\s*\d+\s*é¡µ|Page\s+\d+|å…±\s*\d+\s*é¡µ|\d+\s*/\s*\d+)',
            re.IGNORECASE
        )

    def clean(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬ï¼ˆä¸»å…¥å£ï¼‰

        å‚æ•°ï¼š
            text: åŸå§‹æ–‡æœ¬

        è¿”å›ï¼š
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text:
            return ""

        logger.debug(f"å¼€å§‹æ¸…æ´—æ–‡æœ¬ | åŸå§‹é•¿åº¦: {len(text)}")

        # 1. ä¿®å¤ç¼–ç é—®é¢˜
        if self.fix_encoding:
            text = self._fix_encoding_issues(text)

        # 2. åˆ é™¤URLå’Œé‚®ç®±
        if self.remove_urls:
            text = self.url_pattern.sub('', text)
        if self.remove_emails:
            text = self.email_pattern.sub('', text)

        # 3. æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        text = self._clean_special_chars(text)

        # 4. ä¿®å¤æ¢è¡Œé—®é¢˜
        text = self._fix_line_breaks(text)

        # 5. åˆ é™¤é¡µçœ‰é¡µè„š
        text = self._remove_headers_footers(text)

        # 6. æ ‡å‡†åŒ–ç©ºç™½ç¬¦
        text = self._normalize_whitespace(text)

        # 7. å»é™¤é‡å¤æ®µè½
        if self.remove_duplicates:
            text = self._remove_duplicate_paragraphs(text)

        # 8. æœ€ç»ˆæ¸…ç†
        text = self._final_cleanup(text)

        logger.debug(f"æ–‡æœ¬æ¸…æ´—å®Œæˆ | æ¸…æ´—åé•¿åº¦: {len(text)}")

        return text.strip()

    def clean_batch(self, texts: List[str]) -> List[str]:
        """
        æ‰¹é‡æ¸…æ´—æ–‡æœ¬

        å‚æ•°ï¼š
            texts: æ–‡æœ¬åˆ—è¡¨

        è¿”å›ï¼š
            æ¸…æ´—åçš„æ–‡æœ¬åˆ—è¡¨
        """
        return [self.clean(text) for text in texts]

    def _fix_encoding_issues(self, text: str) -> str:
        """
        ä¿®å¤å¸¸è§çš„ç¼–ç é—®é¢˜

        å¤„ç†ï¼š
        - UTF-8ç¼–ç é”™è¯¯
        - å…¨è§’/åŠè§’æ··ç”¨
        - ç‰¹æ®Šå­—ç¬¦æ›¿æ¢
        """
        # å…¨è§’è½¬åŠè§’ï¼ˆé™¤äº†ä¸­æ–‡æ ‡ç‚¹ï¼‰
        replacements = {
            'ã€€': ' ',  # å…¨è§’ç©ºæ ¼
            'ï¼ˆ': '(',
            'ï¼‰': ')',
            'ã€': '[',
            'ã€‘': ']',
            'ã€Š': '<',
            'ã€‹': '>',
            '"': '"',
            '"': '"',
            ''': "'",
            ''': "'",
        }

        for old, new in replacements.items():
            text = text.replace(old, new)

        # ç§»é™¤é›¶å®½å­—ç¬¦
        zero_width_chars = [
            '\u200b',  # é›¶å®½ç©ºæ ¼
            '\u200c',  # é›¶å®½éè¿æ¥ç¬¦
            '\u200d',  # é›¶å®½è¿æ¥ç¬¦
            '\ufeff',  # é›¶å®½éæ¢è¡Œç¬¦
        ]
        for char in zero_width_chars:
            text = text.replace(char, '')

        return text

    def _clean_special_chars(self, text: str) -> str:
        """
        æ¸…ç†ç‰¹æ®Šå­—ç¬¦

        ä¿ç•™ï¼š
        - ä¸­è‹±æ–‡å­—ç¬¦
        - æ•°å­—
        - å¸¸ç”¨æ ‡ç‚¹ç¬¦å·
        - æ¢è¡Œç¬¦
        """
        # åˆ é™¤æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦ï¼‰
        text = ''.join(
            char for char in text
            if char == '\n' or char == '\t' or not (0 <= ord(char) < 32)
        )

        # ä¿®å¤è¿ç»­æ ‡ç‚¹ç¬¦å·
        text = self.punct_pattern.sub(r'\1', text)

        return text

    def _fix_line_breaks(self, text: str) -> str:
        """
        ä¿®å¤æ¢è¡Œé—®é¢˜

        å¤„ç†ï¼š
        1. PDFä¸­å¸¸è§çš„å•è¯æ–­è¡Œ
        2. ä¸å¿…è¦çš„æ¢è¡Œ
        3. ä¸­æ–‡æ–­è¡Œ
        """
        lines = text.split('\n')
        fixed_lines = []

        i = 0
        while i < len(lines):
            line = lines[i].strip()

            # è·³è¿‡ç©ºè¡Œ
            if not line:
                fixed_lines.append('')
                i += 1
                continue

            # è·³è¿‡å¤ªçŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯é¡µç æˆ–å™ªå£°ï¼‰
            if len(line) < self.min_line_length:
                i += 1
                continue

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸ä¸‹ä¸€è¡Œåˆå¹¶
            if i < len(lines) - 1:
                next_line = lines[i + 1].strip()

                # å¦‚æœå½“å‰è¡Œä»¥è¿å­—ç¬¦ç»“å°¾ï¼ˆè‹±æ–‡å•è¯æ–­è¡Œï¼‰
                if line.endswith('-') and next_line and next_line[0].islower():
                    line = line[:-1] + next_line  # åˆå¹¶å¹¶åˆ é™¤è¿å­—ç¬¦
                    i += 2
                    fixed_lines.append(line)
                    continue

                # å¦‚æœå½“å‰è¡Œä¸æ˜¯ä»¥å¥å·ç»“å°¾ï¼Œä¸”ä¸‹ä¸€è¡Œä¸æ˜¯æ ‡é¢˜
                # ï¼ˆä¸­æ–‡æ–‡æœ¬å¸¸è§çš„ä¸å¿…è¦æ¢è¡Œï¼‰
                if (
                        line and
                        not line[-1] in 'ã€‚ï¼ï¼Ÿ.!?'
                        and next_line
                        and not self._is_heading(next_line)
                ):
                    # åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶
                    if self._should_merge_lines(line, next_line):
                        line = line + next_line
                        i += 2
                        fixed_lines.append(line)
                        continue

            fixed_lines.append(line)
            i += 1

        return '\n'.join(fixed_lines)

    def _should_merge_lines(self, line1: str, line2: str) -> bool:
        """
        åˆ¤æ–­ä¸¤è¡Œæ˜¯å¦åº”è¯¥åˆå¹¶

        é€»è¾‘ï¼š
        - å¦‚æœç¬¬ä¸€è¡Œå¾ˆçŸ­ï¼ˆ<50å­—ç¬¦ï¼‰ä¸”ä¸æ˜¯å®Œæ•´å¥å­
        - å¦‚æœç¬¬äºŒè¡Œä»¥å°å†™å­—æ¯å¼€å¤´ï¼ˆè‹±æ–‡ï¼‰
        - å¦‚æœéƒ½æ˜¯ä¸­æ–‡ä¸”æ²¡æœ‰æ˜æ˜¾çš„æ®µè½åˆ†éš”ç¬¦
        """
        # è‹±æ–‡ï¼šä¸‹ä¸€è¡Œä»¥å°å†™å­—æ¯å¼€å¤´
        if line2 and line2[0].islower():
            return True

        # ä¸­æ–‡ï¼šä¸¤è¡Œéƒ½è¾ƒçŸ­ä¸”æ²¡æœ‰å¥å·
        if (
                len(line1) < 50
                and len(line2) < 50
                and not line1.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ'))
        ):
            return True

        return False

    def _is_heading(self, line: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜

        ç‰¹å¾ï¼š
        - ä»¥æ•°å­—å¼€å¤´ï¼ˆ1.ã€ä¸€ã€ç¬¬ä¸€ç« ç­‰ï¼‰
        - å…¨éƒ¨å¤§å†™ï¼ˆè‹±æ–‡ï¼‰
        - è¾ƒçŸ­ï¼ˆ<30å­—ç¬¦ï¼‰
        """
        if not line:
            return False

        # ç« èŠ‚æ ‡é¢˜æ¨¡å¼
        heading_patterns = [
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+[ç« èŠ‚æ¡]',
            r'^\d+[\.\s]',
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.\ã€]',
        ]

        for pattern in heading_patterns:
            if re.match(pattern, line):
                return True

        # è‹±æ–‡ï¼šå…¨å¤§å†™ä¸”è¾ƒçŸ­
        if line.isupper() and len(line) < 30:
            return True

        return False

    def _remove_headers_footers(self, text: str) -> str:
        """
        åˆ é™¤é¡µçœ‰é¡µè„š

        å¸¸è§æ¨¡å¼ï¼š
        - é¡µç ï¼ˆç¬¬1é¡µã€Page 1ï¼‰
        - æ–‡æ¡£æ ‡é¢˜é‡å¤
        """
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()

            # åˆ é™¤çº¯é¡µç è¡Œ
            if self.page_number_pattern.match(line):
                continue

            # åˆ é™¤é¡µçœ‰é¡µè„šæ ‡è®°
            if self.header_footer_pattern.search(line) and len(line) < 50:
                continue

            cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

    def _normalize_whitespace(self, text: str) -> str:
        """
        æ ‡å‡†åŒ–ç©ºç™½ç¬¦

        å¤„ç†ï¼š
        1. å¤šä¸ªç©ºæ ¼å˜ä¸ºå•ä¸ª
        2. å¤šä¸ªæ¢è¡Œå˜ä¸ºåŒæ¢è¡Œï¼ˆæ®µè½åˆ†éš”ï¼‰
        3. åˆ é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼
        """
        # æ¯è¡Œå»é™¤é¦–å°¾ç©ºæ ¼
        lines = [line.strip() for line in text.split('\n')]

        # åˆå¹¶è¿ç»­ç©ºè¡Œï¼ˆæœ€å¤šä¿ç•™ä¸€ä¸ªç©ºè¡Œï¼‰
        cleaned_lines = []
        prev_empty = False

        for line in lines:
            if not line:
                if not prev_empty:
                    cleaned_lines.append('')
                prev_empty = True
            else:
                # è¡Œå†…å¤šä¸ªç©ºæ ¼å˜ä¸ºå•ä¸ª
                line = self.whitespace_pattern.sub(' ', line)
                cleaned_lines.append(line)
                prev_empty = False

        return '\n'.join(cleaned_lines)

    def _remove_duplicate_paragraphs(self, text: str) -> str:
        """
        å»é™¤é‡å¤æ®µè½

        æ³¨æ„ï¼š
        - åªåˆ é™¤å®Œå…¨ç›¸åŒçš„æ®µè½
        - ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°
        """
        paragraphs = text.split('\n\n')
        seen = set()
        unique_paragraphs = []

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # ä½¿ç”¨æ®µè½çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºæŒ‡çº¹ï¼ˆé¿å…å†…å­˜å ç”¨è¿‡å¤§ï¼‰
            fingerprint = para[:100]

            if fingerprint not in seen:
                seen.add(fingerprint)
                unique_paragraphs.append(para)

        return '\n\n'.join(unique_paragraphs)

    def _final_cleanup(self, text: str) -> str:
        """
        æœ€ç»ˆæ¸…ç†

        ç¡®ä¿æ–‡æœ¬æ ¼å¼è§„èŒƒ
        """
        # åˆ é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½
        text = text.strip()

        # ç¡®ä¿æ®µè½ä¹‹é—´æœ‰æ˜ç¡®åˆ†éš”
        text = re.sub(r'\n{3,}', '\n\n', text)

        return text


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from services.document.cleaner import DocumentCleaner

# 1. åŸºç¡€ä½¿ç”¨
cleaner = DocumentCleaner()

raw_text = '''
ç¬¬ 1 é¡µ

æ ‡    é¢˜
è¿™æ˜¯ä¸€æ®µæ–‡æœ¬ï¼ŒåŒ…å«äº†å¾ˆå¤š
ä¸å¿…è¦çš„æ¢è¡Œã€‚
è¿™æ˜¯URL: https://example.com
é‚®ç®±: test@example.com

ç¬¬ä¸€ç«   æ¦‚è¿°
è¿™æ˜¯æ­£æ–‡å†…å®¹ã€‚ã€‚ã€‚å¤šä½™çš„æ ‡ç‚¹

ç¬¬ 2 é¡µ
'''

cleaned = cleaner.clean(raw_text)
print(cleaned)


# 2. è‡ªå®šä¹‰é…ç½®
cleaner = DocumentCleaner(
    remove_urls=False,      # ä¿ç•™URL
    remove_emails=False,    # ä¿ç•™é‚®ç®±
    min_line_length=5       # æœ€å°è¡Œé•¿åº¦ä¸º5
)

cleaned = cleaner.clean(raw_text)


# 3. æ‰¹é‡æ¸…æ´—
texts = [text1, text2, text3]
cleaned_texts = cleaner.clean_batch(texts)
"""