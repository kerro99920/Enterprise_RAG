"""
========================================
æ–‡æ¡£å…ƒæ•°æ®æå–å™¨
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- ä»æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–å…ƒæ•°æ®
- è‡ªåŠ¨è¯†åˆ«æ–‡æ¡£å±æ€§
- å¢å¼ºæ£€ç´¢å’Œè¿‡æ»¤èƒ½åŠ›

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æå–æ–‡æ¡£æ ‡é¢˜ã€ä½œè€…ç­‰åŸºç¡€ä¿¡æ¯
2. è¯†åˆ«æ–‡æ¡£ç±»å‹å’Œé¢†åŸŸ
3. æå–å…³é”®è¯å’Œæ‘˜è¦
4. ç”Ÿæˆæ–‡æ¡£æŒ‡çº¹

========================================
"""

import re
import hashlib
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from pathlib import Path

import jieba
import jieba.analyse
from loguru import logger


class MetadataExtractor:
    """
    å…ƒæ•°æ®æå–å™¨

    ğŸ”§ æå–å†…å®¹ï¼š
    - åŸºç¡€ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸï¼‰
    - æ–‡æ¡£åˆ†ç±»ï¼ˆç±»å‹ã€é¢†åŸŸï¼‰
    - å…³é”®è¯å’Œæ ‡ç­¾
    - ç»Ÿè®¡ä¿¡æ¯ï¼ˆå­—æ•°ã€æ®µè½æ•°ï¼‰

    ğŸ’¡ åº”ç”¨åœºæ™¯ï¼š
    - æ–‡æ¡£ç®¡ç†
    - æƒé™æ§åˆ¶
    - æ™ºèƒ½æ£€ç´¢
    - æ•°æ®åˆ†æ
    """

    # æ–‡æ¡£ç±»å‹è¯†åˆ«å…³é”®è¯
    DOC_TYPE_KEYWORDS = {
        'regulation': ['è§„èŒƒ', 'æ ‡å‡†', 'GB', 'JGJ', 'å›½æ ‡', 'è¡Œæ ‡'],
        'contract': ['åˆåŒ', 'åè®®', 'ç”²æ–¹', 'ä¹™æ–¹', 'ç­¾è®¢'],
        'report': ['æŠ¥å‘Š', 'æ€»ç»“', 'åˆ†æ', 'è°ƒç ”', 'æ±‡æŠ¥'],
        'manual': ['æ‰‹å†Œ', 'æŒ‡å—', 'è¯´æ˜ä¹¦', 'æ“ä½œ', 'ä½¿ç”¨'],
        'proposal': ['æ–¹æ¡ˆ', 'è®¡åˆ’', 'ææ¡ˆ', 'å»ºè®®'],
        'notice': ['é€šçŸ¥', 'å…¬å‘Š', 'å…¬ç¤º', 'å£°æ˜']
    }

    # é¢†åŸŸè¯†åˆ«å…³é”®è¯
    DOMAIN_KEYWORDS = {
        'construction': ['å»ºç­‘', 'æ–½å·¥', 'å·¥ç¨‹', 'ç»“æ„', 'å»ºé€ '],
        'legal': ['æ³•å¾‹', 'æ³•è§„', 'æ¡ä¾‹', 'æ³•æ¡', 'å¸æ³•'],
        'finance': ['è´¢åŠ¡', 'ä¼šè®¡', 'ç¨åŠ¡', 'å®¡è®¡', 'é‡‘è'],
        'technology': ['æŠ€æœ¯', 'ç§‘æŠ€', 'ç ”å‘', 'ç®—æ³•', 'ç³»ç»Ÿ'],
        'management': ['ç®¡ç†', 'è¿è¥', 'ç»„ç»‡', 'è¡Œæ”¿', 'äººåŠ›']
    }

    def __init__(self):
        """åˆå§‹åŒ–å…ƒæ•°æ®æå–å™¨"""
        # åˆå§‹åŒ–jiebaåˆ†è¯ï¼ˆç”¨äºå…³é”®è¯æå–ï¼‰
        jieba.setLogLevel(jieba.logging.INFO)

    def extract(
            self,
            text: str,
            file_path: Optional[str] = None,
            doc_metadata: Optional[Dict] = None
    ) -> Dict:
        """
        æå–å…ƒæ•°æ®ï¼ˆä¸»å…¥å£ï¼‰

        å‚æ•°ï¼š
            text: æ–‡æ¡£æ–‡æœ¬
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            doc_metadata: å·²æœ‰çš„æ–‡æ¡£å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰

        è¿”å›ï¼š
            å®Œæ•´çš„å…ƒæ•°æ®å­—å…¸
        """
        logger.debug("å¼€å§‹æå–å…ƒæ•°æ®")

        metadata = {}

        # 1. åŸºç¡€ä¿¡æ¯
        metadata.update(self._extract_basic_info(text, file_path))

        # 2. æ–‡æ¡£åˆ†ç±»
        metadata.update(self._extract_classification(text))

        # 3. å…³é”®è¯æå–
        metadata.update(self._extract_keywords(text))

        # 4. ç»Ÿè®¡ä¿¡æ¯
        metadata.update(self._extract_statistics(text))

        # 5. æ–‡æ¡£æŒ‡çº¹
        metadata['doc_fingerprint'] = self._generate_fingerprint(text)

        # 6. åˆå¹¶å·²æœ‰å…ƒæ•°æ®
        if doc_metadata:
            metadata.update(doc_metadata)

        # 7. æ·»åŠ æå–æ—¶é—´
        metadata['metadata_extracted_at'] = datetime.now().isoformat()

        logger.debug(f"å…ƒæ•°æ®æå–å®Œæˆ | å…³é”®è¯æ•°: {len(metadata.get('keywords', []))}")

        return metadata

    def _extract_basic_info(
            self,
            text: str,
            file_path: Optional[str]
    ) -> Dict:
        """
        æå–åŸºç¡€ä¿¡æ¯

        æå–å†…å®¹ï¼š
        - æ ‡é¢˜ï¼ˆä»æ–‡ä»¶åæˆ–æ–‡æ¡£é¦–è¡Œï¼‰
        - æ–‡ä»¶ä¿¡æ¯
        - æ—¥æœŸ
        """
        info = {}

        # 1. æ ‡é¢˜
        title = self._extract_title(text, file_path)
        if title:
            info['title'] = title

        # 2. æ–‡ä»¶ä¿¡æ¯
        if file_path:
            path = Path(file_path)
            info['filename'] = path.name
            info['file_extension'] = path.suffix
            info['file_stem'] = path.stem

        # 3. æ—¥æœŸæå–
        dates = self._extract_dates(text)
        if dates:
            info['extracted_dates'] = dates
            info['latest_date'] = max(dates)

        # 4. ç¼–å·æå–ï¼ˆå¦‚æ–‡ä»¶ç¼–å·ã€åˆåŒç¼–å·ï¼‰
        doc_number = self._extract_document_number(text)
        if doc_number:
            info['document_number'] = doc_number

        return info

    def _extract_title(
            self,
            text: str,
            file_path: Optional[str]
    ) -> Optional[str]:
        """
        æå–æ–‡æ¡£æ ‡é¢˜

        ä¼˜å…ˆçº§ï¼š
        1. æ–‡æ¡£ç¬¬ä¸€è¡Œï¼ˆå¦‚æœæ˜¯æ ‡é¢˜æ ¼å¼ï¼‰
        2. æ–‡ä»¶å
        """
        # å°è¯•ä»æ–‡æœ¬ç¬¬ä¸€è¡Œæå–
        lines = text.split('\n')
        for line in lines[:5]:  # åªçœ‹å‰5è¡Œ
            line = line.strip()
            if not line:
                continue

            # å¦‚æœæ˜¯çŸ­è¡Œä¸”ä¸åŒ…å«è¿‡å¤šæ ‡ç‚¹ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜
            if 5 < len(line) < 100 and line.count('ã€‚') < 2:
                return line

        # ä»æ–‡ä»¶åæå–
        if file_path:
            return Path(file_path).stem

        return None

    def _extract_classification(self, text: str) -> Dict:
        """
        æå–æ–‡æ¡£åˆ†ç±»ä¿¡æ¯

        è¿”å›ï¼š
            {
                'doc_type': str,      # æ–‡æ¡£ç±»å‹
                'domain': str,        # æ‰€å±é¢†åŸŸ
                'confidence': float   # åˆ†ç±»ç½®ä¿¡åº¦
            }
        """
        classification = {}

        # æ–‡æ¡£ç±»å‹è¯†åˆ«
        doc_type, type_confidence = self._classify_by_keywords(
            text,
            self.DOC_TYPE_KEYWORDS
        )
        if doc_type:
            classification['doc_type'] = doc_type
            classification['type_confidence'] = type_confidence

        # é¢†åŸŸè¯†åˆ«
        domain, domain_confidence = self._classify_by_keywords(
            text,
            self.DOMAIN_KEYWORDS
        )
        if domain:
            classification['domain'] = domain
            classification['domain_confidence'] = domain_confidence

        return classification

    def _classify_by_keywords(
            self,
            text: str,
            keyword_dict: Dict[str, List[str]]
    ) -> Tuple[Optional[str], float]:
        """
        åŸºäºå…³é”®è¯çš„åˆ†ç±»

        è¿”å›ï¼š
            (åˆ†ç±»æ ‡ç­¾, ç½®ä¿¡åº¦)
        """
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å…³é”®è¯å‡ºç°æ¬¡æ•°
        scores = {}

        for category, keywords in keyword_dict.items():
            count = 0
            for keyword in keywords:
                count += text.count(keyword)
            scores[category] = count

        # æ‰¾å‡ºå¾—åˆ†æœ€é«˜çš„ç±»åˆ«
        if not scores or max(scores.values()) == 0:
            return None, 0.0

        best_category = max(scores, key=scores.get)
        total_count = sum(scores.values())
        confidence = scores[best_category] / total_count if total_count > 0 else 0

        return best_category, confidence

    def _extract_keywords(
            self,
            text: str,
            top_k: int = 10
    ) -> Dict:
        """
        æå–å…³é”®è¯

        å‚æ•°ï¼š
            text: æ–‡æœ¬
            top_k: æå–å‰kä¸ªå…³é”®è¯

        è¿”å›ï¼š
            {
                'keywords': List[str],           # å…³é”®è¯åˆ—è¡¨
                'keyword_weights': Dict[str, float]  # å…³é”®è¯æƒé‡
            }
        """
        # ä½¿ç”¨jiebaçš„TF-IDFæå–å…³é”®è¯
        try:
            keywords_with_weights = jieba.analyse.extract_tags(
                text,
                topK=top_k,
                withWeight=True
            )

            keywords = [kw for kw, weight in keywords_with_weights]
            keyword_weights = {kw: weight for kw, weight in keywords_with_weights}

            return {
                'keywords': keywords,
                'keyword_weights': keyword_weights
            }
        except Exception as e:
            logger.warning(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            return {
                'keywords': [],
                'keyword_weights': {}
            }

    def _extract_statistics(self, text: str) -> Dict:
        """
        æå–ç»Ÿè®¡ä¿¡æ¯

        è¿”å›ï¼š
            {
                'char_count': int,        # å­—ç¬¦æ•°
                'word_count': int,        # è¯æ•°
                'line_count': int,        # è¡Œæ•°
                'paragraph_count': int,   # æ®µè½æ•°
                'avg_line_length': float, # å¹³å‡è¡Œé•¿
                'language': str           # ä¸»è¦è¯­è¨€
            }
        """
        stats = {}

        # å­—ç¬¦æ•°
        stats['char_count'] = len(text)

        # è¡Œæ•°å’Œæ®µè½æ•°
        lines = [line for line in text.split('\n') if line.strip()]
        paragraphs = [p for p in text.split('\n\n') if p.strip()]
        stats['line_count'] = len(lines)
        stats['paragraph_count'] = len(paragraphs)

        # å¹³å‡è¡Œé•¿
        if lines:
            stats['avg_line_length'] = sum(len(line) for line in lines) / len(lines)
        else:
            stats['avg_line_length'] = 0

        # è¯æ•°ï¼ˆä¸­æ–‡æŒ‰å­—ï¼Œè‹±æ–‡æŒ‰å•è¯ï¼‰
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
        stats['word_count'] = chinese_chars + english_words

        # è¯­è¨€æ£€æµ‹
        if chinese_chars > english_words * 5:
            stats['language'] = 'chinese'
        elif english_words > chinese_chars:
            stats['language'] = 'english'
        else:
            stats['language'] = 'mixed'

        return stats

    def _extract_dates(self, text: str) -> List[str]:
        """
        æå–æ–‡æ¡£ä¸­çš„æ—¥æœŸ

        æ”¯æŒæ ¼å¼ï¼š
        - 2024å¹´1æœˆ1æ—¥
        - 2024-01-01
        - 2024/01/01
        """
        date_patterns = [
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
            r'\d{4}-\d{1,2}-\d{1,2}',
            r'\d{4}/\d{1,2}/\d{1,2}',
        ]

        dates = []
        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            dates.extend(matches)

        # å»é‡å¹¶æ’åº
        dates = sorted(set(dates))

        return dates

    def _extract_document_number(self, text: str) -> Optional[str]:
        """
        æå–æ–‡æ¡£ç¼–å·

        å¸¸è§æ ¼å¼ï¼š
        - GB50009-2012
        - JGJ 123-2020
        - [2024]001å·
        """
        patterns = [
            r'GB\s*\d+[-â€“]\d{4}',
            r'JGJ\s*\d+[-â€“]\d{4}',
            r'\[\d{4}\]\d+å·',
            r'No\.\s*\d+',
        ]

        for pattern in patterns:
            match = re.search(pattern, text[:1000])  # åªåœ¨å¼€å¤´æœç´¢
            if match:
                return match.group(0)

        return None

    def _generate_fingerprint(self, text: str) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£æŒ‡çº¹ï¼ˆç”¨äºå»é‡å’Œæ¯”å¯¹ï¼‰

        ä½¿ç”¨MD5å“ˆå¸Œå‰1000ä¸ªå­—ç¬¦
        """
        sample = text[:1000].encode('utf-8')
        return hashlib.md5(sample).hexdigest()


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from services.document.metadata import MetadataExtractor

# 1. åŸºç¡€ä½¿ç”¨
extractor = MetadataExtractor()

text = '''
å»ºç­‘ç»“æ„è·è½½è§„èŒƒ GB50009-2012

ç¬¬ä¸€ç«  æ€»åˆ™

1.0.1 ä¸ºäº†åœ¨å»ºç­‘ç»“æ„è®¾è®¡ä¸­åˆç†ç¡®å®šè·è½½...
'''

metadata = extractor.extract(text, file_path="GB50009-2012.pdf")

print(f"æ ‡é¢˜: {metadata.get('title')}")
print(f"æ–‡æ¡£ç±»å‹: {metadata.get('doc_type')}")
print(f"é¢†åŸŸ: {metadata.get('domain')}")
print(f"å…³é”®è¯: {metadata.get('keywords')}")
print(f"å­—ç¬¦æ•°: {metadata.get('char_count')}")


# 2. å¸¦å·²æœ‰å…ƒæ•°æ®
existing_metadata = {
    'author': 'ä¸­åäººæ°‘å…±å’Œå›½ä½æˆ¿å’ŒåŸä¹¡å»ºè®¾éƒ¨',
    'publish_date': '2012-05-01'
}

metadata = extractor.extract(
    text,
    file_path="GB50009-2012.pdf",
    doc_metadata=existing_metadata
)
"""