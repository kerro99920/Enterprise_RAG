"""
========================================
å“ˆå¸Œå·¥å…·å‡½æ•°
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- æ–‡ä»¶å’Œæ–‡æœ¬å“ˆå¸Œè®¡ç®—
- å†…å®¹å»é‡
- æŒ‡çº¹ç”Ÿæˆ

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. MD5/SHA256 å“ˆå¸Œ
2. æ–‡ä»¶æŒ‡çº¹
3. å†…å®¹å»é‡æ£€æµ‹
4. ç›¸ä¼¼åº¦è®¡ç®—

========================================
"""

import hashlib
import os
from pathlib import Path
from typing import Union, List, Optional
import json

from loguru import logger


def md5_hash(content: Union[str, bytes]) -> str:
    """
    è®¡ç®— MD5 å“ˆå¸Œå€¼

    å‚æ•°ï¼š
        content: å­—ç¬¦ä¸²æˆ–å­—èŠ‚å†…å®¹

    è¿”å›ï¼š
        str: 32ä½åå…­è¿›åˆ¶å“ˆå¸Œå€¼
    """
    if isinstance(content, str):
        content = content.encode('utf-8')

    return hashlib.md5(content).hexdigest()


def sha256_hash(content: Union[str, bytes]) -> str:
    """
    è®¡ç®— SHA256 å“ˆå¸Œå€¼

    å‚æ•°ï¼š
        content: å­—ç¬¦ä¸²æˆ–å­—èŠ‚å†…å®¹

    è¿”å›ï¼š
        str: 64ä½åå…­è¿›åˆ¶å“ˆå¸Œå€¼
    """
    if isinstance(content, str):
        content = content.encode('utf-8')

    return hashlib.sha256(content).hexdigest()


def file_md5(file_path: Union[str, Path], chunk_size: int = 8192) -> str:
    """
    è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼

    å‚æ•°ï¼š
        file_path: æ–‡ä»¶è·¯å¾„
        chunk_size: åˆ†å—å¤§å°ï¼ˆç”¨äºå¤§æ–‡ä»¶ï¼‰

    è¿”å›ï¼š
        str: 32ä½åå…­è¿›åˆ¶å“ˆå¸Œå€¼
    """
    md5 = hashlib.md5()

    with open(file_path, 'rb') as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            md5.update(chunk)

    return md5.hexdigest()


def file_sha256(file_path: Union[str, Path], chunk_size: int = 8192) -> str:
    """
    è®¡ç®—æ–‡ä»¶çš„ SHA256 å“ˆå¸Œå€¼

    å‚æ•°ï¼š
        file_path: æ–‡ä»¶è·¯å¾„
        chunk_size: åˆ†å—å¤§å°ï¼ˆç”¨äºå¤§æ–‡ä»¶ï¼‰

    è¿”å›ï¼š
        str: 64ä½åå…­è¿›åˆ¶å“ˆå¸Œå€¼
    """
    sha256 = hashlib.sha256()

    with open(file_path, 'rb') as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            sha256.update(chunk)

    return sha256.hexdigest()


def content_fingerprint(
    text: str,
    length: int = 16,
    normalize: bool = True
) -> str:
    """
    ç”Ÿæˆæ–‡æœ¬å†…å®¹æŒ‡çº¹

    ç”¨äºå¿«é€Ÿæ¯”è¾ƒä¸¤æ®µæ–‡æœ¬æ˜¯å¦ç›¸åŒ

    å‚æ•°ï¼š
        text: æ–‡æœ¬å†…å®¹
        length: æŒ‡çº¹é•¿åº¦ï¼ˆæˆªå–å“ˆå¸Œå€¼çš„å‰Nä½ï¼‰
        normalize: æ˜¯å¦æ ‡å‡†åŒ–æ–‡æœ¬ï¼ˆå»é™¤ç©ºæ ¼ã€æ¢è¡Œç­‰ï¼‰

    è¿”å›ï¼š
        str: å†…å®¹æŒ‡çº¹
    """
    if normalize:
        # æ ‡å‡†åŒ–ï¼šå»é™¤ç©ºæ ¼ã€æ¢è¡Œã€è½¬å°å†™
        text = ''.join(text.split()).lower()

    full_hash = md5_hash(text)
    return full_hash[:length]


def document_fingerprint(
    text: str,
    sample_size: int = 1000
) -> str:
    """
    ç”Ÿæˆæ–‡æ¡£æŒ‡çº¹

    åªé‡‡æ ·æ–‡æ¡£çš„å¼€å¤´éƒ¨åˆ†ï¼Œç”¨äºå¿«é€Ÿå»é‡

    å‚æ•°ï¼š
        text: æ–‡æ¡£å†…å®¹
        sample_size: é‡‡æ ·å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰

    è¿”å›ï¼š
        str: æ–‡æ¡£æŒ‡çº¹
    """
    sample = text[:sample_size]
    return content_fingerprint(sample)


def chunk_fingerprint(
    text: str,
    doc_id: str,
    chunk_index: int
) -> str:
    """
    ç”Ÿæˆæ–‡æ¡£å—æŒ‡çº¹

    ç»“åˆæ–‡æ¡£IDã€å—ç´¢å¼•å’Œå†…å®¹ç”Ÿæˆå”¯ä¸€æŒ‡çº¹

    å‚æ•°ï¼š
        text: å—å†…å®¹
        doc_id: æ–‡æ¡£ID
        chunk_index: å—ç´¢å¼•

    è¿”å›ï¼š
        str: å—æŒ‡çº¹
    """
    combined = f"{doc_id}:{chunk_index}:{text[:200]}"
    return md5_hash(combined)


def is_duplicate(
    fingerprint: str,
    existing_fingerprints: List[str]
) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦é‡å¤

    å‚æ•°ï¼š
        fingerprint: å¾…æ£€æŸ¥çš„æŒ‡çº¹
        existing_fingerprints: å·²å­˜åœ¨çš„æŒ‡çº¹åˆ—è¡¨

    è¿”å›ï¼š
        bool: æ˜¯å¦é‡å¤
    """
    return fingerprint in existing_fingerprints


def compute_similarity_hash(
    text: str,
    n_features: int = 128
) -> List[int]:
    """
    è®¡ç®— SimHashï¼ˆç›¸ä¼¼å“ˆå¸Œï¼‰

    ç”¨äºæ£€æµ‹è¿‘ä¼¼é‡å¤çš„æ–‡æœ¬

    å‚æ•°ï¼š
        text: æ–‡æœ¬å†…å®¹
        n_features: ç‰¹å¾æ•°é‡

    è¿”å›ï¼š
        List[int]: SimHash å‘é‡
    """
    import re

    # åˆ†è¯
    words = re.findall(r'\w+', text.lower())

    # åˆå§‹åŒ–å‘é‡
    v = [0] * n_features

    for word in words:
        # è®¡ç®—è¯çš„å“ˆå¸Œ
        word_hash = int(md5_hash(word), 16)

        for i in range(n_features):
            bitmask = 1 << i
            if word_hash & bitmask:
                v[i] += 1
            else:
                v[i] -= 1

    # è½¬æ¢ä¸ºäºŒè¿›åˆ¶æŒ‡çº¹
    fingerprint = [1 if x > 0 else 0 for x in v]

    return fingerprint


def hamming_distance(hash1: List[int], hash2: List[int]) -> int:
    """
    è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œçš„æ±‰æ˜è·ç¦»

    å‚æ•°ï¼š
        hash1: ç¬¬ä¸€ä¸ªå“ˆå¸Œ
        hash2: ç¬¬äºŒä¸ªå“ˆå¸Œ

    è¿”å›ï¼š
        int: æ±‰æ˜è·ç¦»ï¼ˆä¸åŒä½çš„æ•°é‡ï¼‰
    """
    if len(hash1) != len(hash2):
        raise ValueError("å“ˆå¸Œé•¿åº¦å¿…é¡»ç›¸åŒ")

    return sum(b1 != b2 for b1, b2 in zip(hash1, hash2))


def is_near_duplicate(
    text1: str,
    text2: str,
    threshold: float = 0.9
) -> bool:
    """
    æ£€æµ‹ä¸¤æ®µæ–‡æœ¬æ˜¯å¦è¿‘ä¼¼é‡å¤

    å‚æ•°ï¼š
        text1: ç¬¬ä¸€æ®µæ–‡æœ¬
        text2: ç¬¬äºŒæ®µæ–‡æœ¬
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰

    è¿”å›ï¼š
        bool: æ˜¯å¦è¿‘ä¼¼é‡å¤
    """
    hash1 = compute_similarity_hash(text1)
    hash2 = compute_similarity_hash(text2)

    distance = hamming_distance(hash1, hash2)
    similarity = 1 - (distance / len(hash1))

    return similarity >= threshold


def generate_unique_id(
    prefix: str = "",
    length: int = 8
) -> str:
    """
    ç”Ÿæˆå”¯ä¸€ID

    å‚æ•°ï¼š
        prefix: IDå‰ç¼€
        length: IDé•¿åº¦ï¼ˆä¸å«å‰ç¼€ï¼‰

    è¿”å›ï¼š
        str: å”¯ä¸€ID
    """
    import uuid
    import time

    # ç»“åˆæ—¶é—´æˆ³å’ŒUUID
    unique_str = f"{time.time()}{uuid.uuid4()}"
    hash_value = md5_hash(unique_str)[:length]

    if prefix:
        return f"{prefix}_{hash_value}"
    return hash_value


def hash_dict(data: dict) -> str:
    """
    è®¡ç®—å­—å…¸çš„å“ˆå¸Œå€¼

    å‚æ•°ï¼š
        data: å­—å…¸æ•°æ®

    è¿”å›ï¼š
        str: å“ˆå¸Œå€¼
    """
    # è½¬æ¢ä¸ºæ’åºåçš„ JSON å­—ç¬¦ä¸²
    json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return md5_hash(json_str)


class DeduplicationManager:
    """
    å»é‡ç®¡ç†å™¨

    ç”¨äºç®¡ç†å·²å¤„ç†æ–‡æ¡£çš„æŒ‡çº¹ï¼Œé¿å…é‡å¤å¤„ç†
    """

    def __init__(self, storage_path: Optional[str] = None):
        """
        åˆå§‹åŒ–å»é‡ç®¡ç†å™¨

        å‚æ•°ï¼š
            storage_path: æŒ‡çº¹å­˜å‚¨æ–‡ä»¶è·¯å¾„
        """
        self.fingerprints = set()
        self.storage_path = storage_path

        if storage_path and os.path.exists(storage_path):
            self.load()

    def add(self, fingerprint: str) -> bool:
        """
        æ·»åŠ æŒ‡çº¹

        å‚æ•°ï¼š
            fingerprint: æŒ‡çº¹

        è¿”å›ï¼š
            bool: æ˜¯å¦ä¸ºæ–°æŒ‡çº¹ï¼ˆTrue=æ–°ï¼ŒFalse=é‡å¤ï¼‰
        """
        if fingerprint in self.fingerprints:
            return False

        self.fingerprints.add(fingerprint)
        return True

    def check(self, fingerprint: str) -> bool:
        """
        æ£€æŸ¥æŒ‡çº¹æ˜¯å¦å­˜åœ¨

        å‚æ•°ï¼š
            fingerprint: æŒ‡çº¹

        è¿”å›ï¼š
            bool: æ˜¯å¦å­˜åœ¨
        """
        return fingerprint in self.fingerprints

    def remove(self, fingerprint: str):
        """ç§»é™¤æŒ‡çº¹"""
        self.fingerprints.discard(fingerprint)

    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰æŒ‡çº¹"""
        self.fingerprints.clear()

    def save(self):
        """ä¿å­˜æŒ‡çº¹åˆ°æ–‡ä»¶"""
        if self.storage_path:
            with open(self.storage_path, 'w') as f:
                json.dump(list(self.fingerprints), f)
            logger.debug(f"æŒ‡çº¹å·²ä¿å­˜: {len(self.fingerprints)} ä¸ª")

    def load(self):
        """ä»æ–‡ä»¶åŠ è½½æŒ‡çº¹"""
        if self.storage_path and os.path.exists(self.storage_path):
            with open(self.storage_path, 'r') as f:
                self.fingerprints = set(json.load(f))
            logger.debug(f"æŒ‡çº¹å·²åŠ è½½: {len(self.fingerprints)} ä¸ª")

    def __len__(self):
        return len(self.fingerprints)

    def __contains__(self, fingerprint: str):
        return fingerprint in self.fingerprints


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from utils.hash_utils import *

# 1. è®¡ç®—æ–‡æœ¬å“ˆå¸Œ
text = "è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬"
print(f"MD5: {md5_hash(text)}")
print(f"SHA256: {sha256_hash(text)}")

# 2. è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
print(f"æ–‡ä»¶MD5: {file_md5('document.pdf')}")
print(f"æ–‡ä»¶SHA256: {file_sha256('document.pdf')}")

# 3. ç”Ÿæˆå†…å®¹æŒ‡çº¹
fingerprint = content_fingerprint(text)
print(f"å†…å®¹æŒ‡çº¹: {fingerprint}")

# 4. æ–‡æ¡£æŒ‡çº¹
doc_fp = document_fingerprint(long_text)
print(f"æ–‡æ¡£æŒ‡çº¹: {doc_fp}")

# 5. æ£€æµ‹è¿‘ä¼¼é‡å¤
text1 = "è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬å†…å®¹"
text2 = "è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬å†…å®¹ã€‚"  # åªå¤šäº†ä¸€ä¸ªå¥å·
is_dup = is_near_duplicate(text1, text2, threshold=0.8)
print(f"æ˜¯å¦è¿‘ä¼¼é‡å¤: {is_dup}")

# 6. ç”Ÿæˆå”¯ä¸€ID
unique_id = generate_unique_id(prefix="doc", length=8)
print(f"å”¯ä¸€ID: {unique_id}")  # å¦‚: doc_a1b2c3d4

# 7. ä½¿ç”¨å»é‡ç®¡ç†å™¨
dedup = DeduplicationManager(storage_path="data/fingerprints.json")

# æ·»åŠ æŒ‡çº¹
is_new = dedup.add(fingerprint)
if is_new:
    print("æ–°æ–‡æ¡£ï¼Œå¼€å§‹å¤„ç†")
else:
    print("é‡å¤æ–‡æ¡£ï¼Œè·³è¿‡")

# æ£€æŸ¥æŒ‡çº¹
if fingerprint in dedup:
    print("å·²å­˜åœ¨")

# ä¿å­˜
dedup.save()
"""
