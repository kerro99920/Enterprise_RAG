"""
========================================
æ–‡æœ¬å¤„ç†å·¥å…·
========================================

ğŸ“š æ¨¡å—è¯´æ˜ï¼š
- æ–‡æœ¬é¢„å¤„ç†å·¥å…·
- åˆ†è¯å’Œæ ‡å‡†åŒ–
- æ”¯æŒä¸­è‹±æ–‡æ··åˆ

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä¸­è‹±æ–‡åˆ†è¯
2. åœç”¨è¯è¿‡æ»¤
3. æ–‡æœ¬æ ‡å‡†åŒ–
4. å…³é”®è¯æå–

========================================
"""

import re
from typing import List, Set, Optional

import jieba
import jieba.analyse
from loguru import logger


class TextProcessor:
    """
    æ–‡æœ¬å¤„ç†å™¨

    ğŸ”§ åŠŸèƒ½ï¼š
    - ä¸­æ–‡åˆ†è¯ï¼ˆjiebaï¼‰
    - è‹±æ–‡åˆ†è¯
    - åœç”¨è¯è¿‡æ»¤
    - æ–‡æœ¬æ ‡å‡†åŒ–

    ğŸ’¡ åº”ç”¨åœºæ™¯ï¼š
    - BM25æ£€ç´¢é¢„å¤„ç†
    - å…³é”®è¯æå–
    - æ–‡æœ¬æ¸…æ´—
    """

    # é»˜è®¤ä¸­æ–‡åœç”¨è¯
    DEFAULT_STOPWORDS = {
        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
        'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
        'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'èƒ½', 'é‚£',
        'æ¥', 'ä½†', 'ä¸', 'å¯¹', 'äº', 'ç”±', 'ä»', 'ä»¥', 'å‘', 'ç”¨',
        'æ¯”', 'æˆ–', 'è¢«', 'å› ', 'æ‰€', 'è€Œ', 'åŠ', 'ç­‰', 'ä¸º', 'ä¹‹'
    }

    def __init__(
            self,
            use_stopwords: bool = True,
            custom_stopwords: Optional[Set[str]] = None,
            enable_jieba_userdict: bool = False,
            userdict_path: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨

        å‚æ•°ï¼š
            use_stopwords: æ˜¯å¦ä½¿ç”¨åœç”¨è¯è¿‡æ»¤
            custom_stopwords: è‡ªå®šä¹‰åœç”¨è¯é›†åˆ
            enable_jieba_userdict: æ˜¯å¦å¯ç”¨jiebaè‡ªå®šä¹‰è¯å…¸
            userdict_path: è‡ªå®šä¹‰è¯å…¸è·¯å¾„
        """
        self.use_stopwords = use_stopwords

        # åœç”¨è¯é›†åˆ
        self.stopwords = self.DEFAULT_STOPWORDS.copy()
        if custom_stopwords:
            self.stopwords.update(custom_stopwords)

        # è®¾ç½®jiebaæ—¥å¿—çº§åˆ«
        jieba.setLogLevel(jieba.logging.INFO)

        # åŠ è½½è‡ªå®šä¹‰è¯å…¸
        if enable_jieba_userdict and userdict_path:
            try:
                jieba.load_userdict(userdict_path)
                logger.info(f"åŠ è½½jiebaè‡ªå®šä¹‰è¯å…¸: {userdict_path}")
            except Exception as e:
                logger.warning(f"åŠ è½½è‡ªå®šä¹‰è¯å…¸å¤±è´¥: {e}")

        logger.info("æ–‡æœ¬å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def tokenize(
            self,
            text: str,
            mode: str = 'search'
    ) -> List[str]:
        """
        æ–‡æœ¬åˆ†è¯

        å‚æ•°ï¼š
            text: è¾“å…¥æ–‡æœ¬
            mode: åˆ†è¯æ¨¡å¼
                - 'default': ç²¾ç¡®æ¨¡å¼
                - 'search': æœç´¢å¼•æ“æ¨¡å¼ï¼ˆæ¨èï¼‰
                - 'all': å…¨æ¨¡å¼

        è¿”å›ï¼š
            è¯åˆ—è¡¨
        """
        if not text or not text.strip():
            return []

        # é¢„å¤„ç†
        text = self._preprocess(text)

        # åˆ†è¯
        if mode == 'search':
            tokens = jieba.cut_for_search(text)
        elif mode == 'all':
            tokens = jieba.cut(text, cut_all=True)
        else:  # default
            tokens = jieba.cut(text, cut_all=False)

        tokens = list(tokens)

        # è¿‡æ»¤
        tokens = self._filter_tokens(tokens)

        return tokens

    def tokenize_batch(
            self,
            texts: List[str],
            mode: str = 'search'
    ) -> List[List[str]]:
        """æ‰¹é‡åˆ†è¯"""
        return [self.tokenize(text, mode) for text in texts]

    def extract_keywords(
            self,
            text: str,
            top_k: int = 10,
            method: str = 'tfidf'
    ) -> List[str]:
        """
        æå–å…³é”®è¯

        å‚æ•°ï¼š
            text: è¾“å…¥æ–‡æœ¬
            top_k: æå–æ•°é‡
            method: æå–æ–¹æ³•
                - 'tfidf': TF-IDF
                - 'textrank': TextRank

        è¿”å›ï¼š
            å…³é”®è¯åˆ—è¡¨
        """
        if not text or not text.strip():
            return []

        try:
            if method == 'tfidf':
                keywords = jieba.analyse.extract_tags(
                    text,
                    topK=top_k,
                    withWeight=False
                )
            elif method == 'textrank':
                keywords = jieba.analyse.textrank(
                    text,
                    topK=top_k,
                    withWeight=False
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å…³é”®è¯æå–æ–¹æ³•: {method}")

            return list(keywords)

        except Exception as e:
            logger.error(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            return []

    def _preprocess(self, text: str) -> str:
        """
        æ–‡æœ¬é¢„å¤„ç†

        å¤„ç†ï¼š
        1. è½¬å°å†™
        2. å»é™¤å¤šä½™ç©ºç™½
        3. ä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸ç”¨æ ‡ç‚¹
        """
        # è½¬å°å†™ï¼ˆä¿ç•™ä¸­æ–‡ï¼‰
        text = text.lower()

        # å»é™¤URL
        text = re.sub(r'http[s]?://\S+', '', text)

        # å»é™¤é‚®ç®±
        text = re.sub(r'\S+@\S+', '', text)

        # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼ã€å¸¸ç”¨æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', ' ', text)

        # ç»Ÿä¸€ç©ºç™½ç¬¦
        text = re.sub(r'\s+', ' ', text)

        return text.strip()

    def _filter_tokens(self, tokens: List[str]) -> List[str]:
        """
        è¿‡æ»¤è¯å…ƒ

        è¿‡æ»¤è§„åˆ™ï¼š
        1. åœç”¨è¯
        2. å•å­—ç¬¦ï¼ˆé™¤äº†æœ‰æ„ä¹‰çš„å­—ï¼‰
        3. çº¯æ•°å­—
        4. çº¯ç©ºç™½
        """
        filtered = []

        for token in tokens:
            token = token.strip()

            # è·³è¿‡ç©ºç™½
            if not token:
                continue

            # è·³è¿‡åœç”¨è¯
            if self.use_stopwords and token in self.stopwords:
                continue

            # è·³è¿‡å•å­—ç¬¦ï¼ˆé™¤äº†æœ‰æ„ä¹‰çš„ä¸­æ–‡å­—ï¼‰
            if len(token) == 1 and not self._is_meaningful_char(token):
                continue

            # è·³è¿‡çº¯æ ‡ç‚¹
            if re.match(r'^[.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]+$', token):
                continue

            filtered.append(token)

        return filtered

    def _is_meaningful_char(self, char: str) -> bool:
        """åˆ¤æ–­å•å­—ç¬¦æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆä¸»è¦é’ˆå¯¹ä¸­æ–‡ï¼‰"""
        # ä¸­æ–‡å­—ç¬¦ä¸€èˆ¬éƒ½æœ‰æ„ä¹‰
        if '\u4e00' <= char <= '\u9fa5':
            return True

        # è‹±æ–‡å­—æ¯ï¼ˆA-Z, a-zï¼‰
        if char.isalpha():
            return True

        return False

    def add_stopwords(self, words: List[str]):
        """æ·»åŠ åœç”¨è¯"""
        self.stopwords.update(words)
        logger.info(f"æ·»åŠ  {len(words)} ä¸ªåœç”¨è¯")

    def remove_stopwords(self, words: List[str]):
        """ç§»é™¤åœç”¨è¯"""
        for word in words:
            self.stopwords.discard(word)
        logger.info(f"ç§»é™¤ {len(words)} ä¸ªåœç”¨è¯")

    def get_stopwords(self) -> Set[str]:
        """è·å–å½“å‰åœç”¨è¯é›†åˆ"""
        return self.stopwords.copy()


# =========================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# =========================================
"""
from services.retrieval.text_utils import TextProcessor

# 1. åŸºç¡€åˆ†è¯
processor = TextProcessor()

text = "å»ºç­‘ç»“æ„è·è½½è§„èŒƒGB50009-2012æ˜¯å·¥ç¨‹è®¾è®¡çš„é‡è¦æ ‡å‡†"
tokens = processor.tokenize(text)
print(f"åˆ†è¯ç»“æœ: {tokens}")
# è¾“å‡º: ['å»ºç­‘', 'ç»“æ„', 'è·è½½', 'è§„èŒƒ', 'gb50009', '2012', 'å·¥ç¨‹', 'è®¾è®¡', 'é‡è¦', 'æ ‡å‡†']


# 2. æ‰¹é‡åˆ†è¯
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
tokens_list = processor.tokenize_batch(texts)


# 3. å…³é”®è¯æå–
text = '''
å»ºç­‘ç»“æ„è·è½½è§„èŒƒæ˜¯å»ºç­‘å·¥ç¨‹è®¾è®¡ä¸­çš„åŸºç¡€æ€§æ ‡å‡†ï¼Œ
ä¸»è¦è§„å®šäº†å„ç±»è·è½½çš„å–å€¼æ–¹æ³•å’Œç»„åˆåŸåˆ™ã€‚
'''

keywords_tfidf = processor.extract_keywords(text, top_k=5, method='tfidf')
keywords_textrank = processor.extract_keywords(text, top_k=5, method='textrank')

print(f"TF-IDFå…³é”®è¯: {keywords_tfidf}")
print(f"TextRankå…³é”®è¯: {keywords_textrank}")


# 4. è‡ªå®šä¹‰åœç”¨è¯
custom_stopwords = {'å»ºç­‘', 'å·¥ç¨‹'}
processor = TextProcessor(custom_stopwords=custom_stopwords)

# æˆ–åŠ¨æ€æ·»åŠ 
processor.add_stopwords(['è®¾è®¡', 'æ ‡å‡†'])


# 5. ä¸ä½¿ç”¨åœç”¨è¯
processor = TextProcessor(use_stopwords=False)
tokens = processor.tokenize(text)
"""